{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Meta_DRN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtmycqJ/FDPVYTJbjTnVsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrinmoysarkar/MachineLearningAlgorithms/blob/master/Meta_DRN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz-IIKTox0RB"
      },
      "source": [
        "!pip3 install pytorch-lightning higher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN9Sgn2HxcHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "f8f1cb8d-713a-4563-d4fe-957ad22ae2a2"
      },
      "source": [
        "#@title\n",
        "# generic\n",
        "import functools\n",
        "import os, gc\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "from collections import Counter, OrderedDict\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# numeric computation and plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# deep learning\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch._C import ParameterDict\n",
        "from pytorch_lightning.metrics.functional.classification import iou\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import Optimizer\n",
        "from higher.optim import DifferentiableOptimizer\n",
        "from higher import register_optim\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch import Tensor\n",
        "from torch.nn import Module\n",
        "from torch.types import Device\n",
        "\n",
        "import builtins\n",
        "from typing import Any, Callable, Dict, Mapping, Optional, Tuple, Type, Union, List\n",
        "\n",
        "from copy import deepcopy\n",
        "from collections import OrderedDict\n",
        "\n",
        "import higher\n",
        "from higher.optim import _GroupedGradsType, _torch, _math, _add, _addcdiv, _maybe_mask, DifferentiableAdam\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "from requests.models import Response\n",
        "\n",
        "# seeding for reprocibility\n",
        "seed_everything(1971) # 1971 --> random seed\n",
        "\n",
        "# environment setup\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9uFbUjbxuGq",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\"\"\"Configs for global use\"\"\"\n",
        "global_config = {'seed': 1971}\n",
        "\n",
        "\"\"\"Configs for the dataset options\"\"\"\n",
        "data_config = {\n",
        "    'data_root': '/content/drive/MyDrive/Colab Notebooks/data',\n",
        "    'dataset_name': 'FSS-1000',\n",
        "    'gdrive_file_id': '16TgqOeI_0P41Eh3jWQlxlRXG9KIqtMgI',\n",
        "    'dataset_dir': 'fewshot_data',\n",
        "    'img_height': 224,\n",
        "    'img_width': 224,\n",
        "    'n_classes': 1000,\n",
        "    'n_train_classes': 700,\n",
        "    'n_val_classes': 60,\n",
        "    'n_test_classes': 240,\n",
        "    'shuffle': True,\n",
        "    'num_workers': 4,\n",
        "    'normalize_mean': [0.485, 0.456, 0.406],\n",
        "    'normalize_std': [0.229, 0.224, 0.225],\n",
        "    'batch_size': 1,\n",
        "    'maml': {\n",
        "        'test_shots': 1,\n",
        "        'train_shots': 1,\n",
        "        'n_ways': 5\n",
        "    },\n",
        "    'fomaml': {\n",
        "        'test_shots': 1,\n",
        "        'train_shots': 1,\n",
        "        'n_ways': 5\n",
        "    },\n",
        "    'meta-sgd': {\n",
        "        'test_shots': 1,\n",
        "        'train_shots': 1,\n",
        "        'n_ways': 5\n",
        "    },\n",
        "    'reptile': {\n",
        "        'test_shots': 8,\n",
        "        'train_shots': 5,\n",
        "        'n_ways': 5\n",
        "    }\n",
        "}\n",
        "\n",
        "\"\"\"Configs for the model hyperparameters\"\"\"\n",
        "model_config = {\n",
        "    'head': {\n",
        "        'conv1': {\n",
        "            'in_channels': 3,\n",
        "            'out_channels': 16,\n",
        "            'kernel_size': 3,\n",
        "            'stride': 2,\n",
        "            'padding': 1,\n",
        "            'dilation': 1\n",
        "        },\n",
        "        'bn1': {\n",
        "            'num_features': 16\n",
        "        },\n",
        "        'conv2': {\n",
        "            'in_channels': 16,\n",
        "            'out_channels': 64,\n",
        "            'kernel_size': 3,\n",
        "            'stride': 1,\n",
        "            'padding': 1,\n",
        "            'dilation': 1\n",
        "        },\n",
        "        'bn2': {\n",
        "            'num_features': 64\n",
        "        }\n",
        "    },\n",
        "    'resblocks': {\n",
        "        'resblock1': {\n",
        "            'conv1': {\n",
        "                'in_channels': 64,\n",
        "                'out_channels': 128,\n",
        "                'kernel_size': 3,\n",
        "                'stride': 2,\n",
        "                'padding': 1,\n",
        "                'dilation': 1\n",
        "            },\n",
        "            'conv2': {\n",
        "                'in_channels': 128,\n",
        "                'out_channels': 128,\n",
        "                'kernel_size': 3,\n",
        "                'stride': 1,\n",
        "                'padding': 1,\n",
        "                'dilation': 1\n",
        "            }\n",
        "        },\n",
        "        'resblock2': {\n",
        "            'conv1': {\n",
        "                'in_channels': 128,\n",
        "                'out_channels': 256,\n",
        "                'kernel_size': 3,\n",
        "                'stride': 1,\n",
        "                'padding': 1,\n",
        "                'dilation': 1\n",
        "            },\n",
        "            'conv2': {\n",
        "                'in_channels': 256,\n",
        "                'out_channels': 256,\n",
        "                'kernel_size': 3,\n",
        "                'stride': 1,\n",
        "                'padding': 2,\n",
        "                'dilation': 2\n",
        "            }\n",
        "        },\n",
        "        'resblock3': {\n",
        "            'conv1': {\n",
        "                'in_channels': 256,\n",
        "                'out_channels': 512,\n",
        "                'kernel_size': 3,\n",
        "                'stride': 1,\n",
        "                'padding': 2,\n",
        "                'dilation': 2\n",
        "            },\n",
        "            'conv2': {\n",
        "                'in_channels': 512,\n",
        "                'out_channels': 512,\n",
        "                'kernel_size': 3,\n",
        "                'stride': 1,\n",
        "                'padding': 4,\n",
        "                'dilation': 4\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'reducer': {\n",
        "        'resblock1': {\n",
        "            'in_channels': 64,\n",
        "            'out_channels': 128,\n",
        "            'kernel_size': 1,\n",
        "            'stride': 2,\n",
        "            'padding': 0,\n",
        "            'dilation': 1\n",
        "        },\n",
        "        'resblock2': {\n",
        "            'in_channels': 128,\n",
        "            'out_channels': 256,\n",
        "            'kernel_size': 1,\n",
        "            'stride': 1,\n",
        "            'padding': 0,\n",
        "            'dilation': 1\n",
        "        },\n",
        "        'resblock3': {\n",
        "            'in_channels': 256,\n",
        "            'out_channels': 512,\n",
        "            'kernel_size': 1,\n",
        "            'stride': 1,\n",
        "            'padding': 0,\n",
        "            'dilation': 1\n",
        "        }\n",
        "    },\n",
        "    'degrid': {\n",
        "        'conv1': {\n",
        "            'in_channels': 512,\n",
        "            'out_channels': 512,\n",
        "            'kernel_size': 3,\n",
        "            'stride': 1,\n",
        "            'padding': 2,\n",
        "            'dilation': 2\n",
        "        },\n",
        "        'conv2': {\n",
        "            'in_channels': 512,\n",
        "            'out_channels': 512,\n",
        "            'kernel_size': 3,\n",
        "            'stride': 1,\n",
        "            'padding': 1,\n",
        "            'dilation': 1\n",
        "        }\n",
        "    },\n",
        "    'upsample': {\n",
        "        'conv': {\n",
        "            'in_channels': 512,\n",
        "            'out_channels': 32,\n",
        "            'kernel_size': 3,\n",
        "            'stride': 1,\n",
        "            'padding': 1,\n",
        "            'dilation': 1\n",
        "        },\n",
        "        'pixel_shuffle': {\n",
        "            'upscale_factor': 4\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\"\"\"Configs for training\"\"\"\n",
        "train_config = {\n",
        "    'ngpus': 1,\n",
        "    'metrics': ['iou', 'learner_loss', 'meta_loss'],\n",
        "    'n_epochs': 200,\n",
        "    'maml': {\n",
        "        'learner_lr': 1e-3,\n",
        "        'meta_lr': 1e-3,\n",
        "        'train_steps': 1,\n",
        "        'halve_lr_every': 8,\n",
        "        'lr_reduction_factor': 0.5,\n",
        "        'metric_to_watch': 'mIoU'\n",
        "    },\n",
        "    'fomaml': {\n",
        "        'learner_lr': 1e-3,\n",
        "        'meta_lr': 1e-3,\n",
        "        'train_steps': 1,\n",
        "        'halve_lr_every': 8,\n",
        "        'lr_reduction_factor': 0.5,\n",
        "        'metric_to_watch': 'mIoU'\n",
        "    },\n",
        "    'meta-sgd': {\n",
        "        'learner_lr': 1e-3,\n",
        "        'meta_lr': 1e-3,\n",
        "        'train_steps': 1,\n",
        "        'halve_lr_every': 8,\n",
        "        'lr_reduction_factor': 0.5,\n",
        "        'metric_to_watch': 'iou'\n",
        "    },\n",
        "    'reptile': {\n",
        "        'learner_lr': 1e-3,\n",
        "        'meta_lr': 3e-2,\n",
        "        'train_steps': 5,\n",
        "        'final_meta_lr': 3e-5\n",
        "    }\n",
        "}\n",
        "\n",
        "\"\"\"Configs for utlities and transformations\"\"\"\n",
        "utils_config = {\n",
        "    'transforms': [{\n",
        "        'transform': 'Resize',\n",
        "        'params': {\n",
        "            'height': data_config['img_height'],\n",
        "            'width': data_config['img_width']\n",
        "        }\n",
        "    }, {\n",
        "        'transform': 'HorizontalFlip'\n",
        "    }, {\n",
        "        'transform': 'VerticalFlip'\n",
        "    }, {\n",
        "        'transform': 'ShiftScaleRotate',\n",
        "        'params': {\n",
        "            'shift_limit': 0,\n",
        "            'rotate_limit': 0\n",
        "        }\n",
        "    }, {\n",
        "        'transform': 'RandomBrightnessContrast'\n",
        "    }, {\n",
        "        'transform': 'Normalize',\n",
        "        'params': {\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225]\n",
        "        }\n",
        "    }]\n",
        "}\n",
        "\n",
        "\"\"\"Configs for visualization\"\"\"\n",
        "vis_config = {\n",
        "    'tensorboard': {\n",
        "        'logdir': '/content/drive/MyDrive/Colab Notebooks/logs',\n",
        "        'progress_bar': ['iou', 'learner_loss', 'meta_loss']\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS2sFztlyPrb",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\"\"\"A list of common types used in multiple places.\"\"\"\n",
        "\n",
        "_int = builtins.int\n",
        "_float = builtins.float\n",
        "_tensor = Tensor\n",
        "\n",
        "_opt_int = Optional[_int]\n",
        "_opt_float = Optional[_float]\n",
        "_opt_tensor = Optional[Tensor]\n",
        "\n",
        "_inttuple = Tuple[_int, _int]\n",
        "_floattuple = Tuple[_float, _float]\n",
        "_strtuple = Tuple[str, str]\n",
        "_ttuple = Tuple[Tensor, Tensor]\n",
        "_opt_ttuple = Optional[_ttuple]\n",
        "\n",
        "_intstr = Union[_int, str]\n",
        "_intfloat = Union[_int, _float]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz7eWfBtyXaw"
      },
      "source": [
        "#@title\n",
        "\"\"\"Implementation of the torch dataset\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def download_file_from_google_drive(file_id: str, destination: str) -> None:\n",
        "    print(\"Downloading \", destination.rpartition(\"/\")[-1])\n",
        "    url = \"https://docs.google.com/uc?export=download\"\n",
        "    session = requests.Session()\n",
        "    response = session.get(url, params={\"id\": file_id}, stream=True)\n",
        "    token = get_confirm_token(response)\n",
        "    if token:\n",
        "        params = {\"id\": file_id, \"confirm\": token}\n",
        "        response = session.get(url, params=params, stream=True)\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "\n",
        "def get_confirm_token(response: Response) -> Union[str, None]:\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith(\"download_warning\"):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "\n",
        "def save_response_content(response: Response, destination: str) -> None:\n",
        "    chunk_size = 32768\n",
        "    with open(destination, \"wb\") as f:\n",
        "        pbar = tqdm(total=None)\n",
        "        progress = 0\n",
        "        for chunk in response.iter_content(chunk_size):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                progress += len(chunk)\n",
        "                pbar.update(progress - pbar.n)\n",
        "                f.write(chunk)\n",
        "        pbar.close()\n",
        "\n",
        "\n",
        "def count_parameters(model: Module) -> _int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def get_transforms() -> A.Compose:\n",
        "    transforms = utils_config[\"transforms\"]\n",
        "    ts = []\n",
        "    for t in transforms:\n",
        "        trans = t[\"transform\"]\n",
        "        params = t.get(\"params\", {})\n",
        "        if hasattr(A, trans):\n",
        "            if params is not None:\n",
        "                ts.append(getattr(A, trans)(**params))\n",
        "            else:\n",
        "                ts.append(getattr(A, trans)(**params))\n",
        "    transform = A.Compose([*ts, ToTensor()])\n",
        "    return transform\n",
        "\n",
        "\n",
        "def timer(func: Callable[..., Any]) -> Callable[..., Any]:\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        t1 = time.perf_counter()\n",
        "        retval = func(*args, **kwargs)\n",
        "        t2 = time.perf_counter()\n",
        "        wrapper.time_taken = t2 - t1\n",
        "        print(\"Time taken to run %s: %.2fs\" % (func.__name__, t2 - t1))\n",
        "        return retval\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "class FSSDataset(Dataset):\n",
        "    \"\"\"A subclass of torch.utils.data.Dataset that reads images\n",
        "    from the  dataset folder into the appropriate support and query\n",
        "    sets defined in data_config.\n",
        "    \"\"\"\n",
        "    folder = data_config['dataset_dir']\n",
        "\n",
        "    def __init__(self,\n",
        "                 root: str,\n",
        "                 ways: _int,\n",
        "                 shots: _int,\n",
        "                 test_shots: _int,\n",
        "                 meta_split: Optional[str] = 'train',\n",
        "                 transform: Optional[Any] = None,\n",
        "                 download: Optional[bool] = True):\n",
        "        super().__init__()\n",
        "        assert meta_split in ['train', 'val',\n",
        "                          'test'], \"meta-split must be either 'train',\\\n",
        "                 'val' or 'test'\"\n",
        "\n",
        "        self.ways = ways\n",
        "        self.shots = shots\n",
        "        self.transform = transform\n",
        "        self.test_shots = test_shots\n",
        "        self.meta_split = meta_split\n",
        "        if transform is None:\n",
        "            self.transform = A.Compose([\n",
        "                                        A.Normalize(mean=data_config['normalize_mean'], std=data_config['normalize_std']),\n",
        "                                        ToTensor()\n",
        "                                        ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "        if download:\n",
        "            self.download(root)\n",
        "\n",
        "        self.root = os.path.expanduser(os.path.join(root, self.folder))\n",
        "        all_classes = os.listdir(self.root)\n",
        "\n",
        "        if meta_split == 'train':\n",
        "            self.classes = [all_classes[i] for i in range(data_config['n_train_classes'])]\n",
        "        elif meta_split == 'val':\n",
        "            self.classes = [\n",
        "                            all_classes[i]\n",
        "                            for i in range(data_config['n_train_classes'], data_config['n_train_classes'] +\n",
        "                                           data_config['n_val_classes'])\n",
        "                            ]\n",
        "        else:\n",
        "            self.classes = [\n",
        "                            all_classes[i]\n",
        "                            for i in range(data_config['n_train_classes'] +\n",
        "                                           data_config['n_val_classes'], data_config['n_classes'])\n",
        "                            ]\n",
        "\n",
        "        self.num_classes = len(self.classes)\n",
        "\n",
        "    def thresh_mask(self, mask, thresh=0.5):\n",
        "        thresh = (mask.min() + mask.max()) * thresh\n",
        "        mask = mask > thresh\n",
        "        return mask.long()\n",
        "\n",
        "    def make_batch(self, classes):\n",
        "        shots = self.shots + self.test_shots\n",
        "        batch = torch.zeros((shots, self.ways, 4, 224, 224))\n",
        "\n",
        "        for i in range(shots):\n",
        "            for j, cname in enumerate(classes):\n",
        "                img_id = str(random.choice(list(range(1, 11))))\n",
        "                img = Image.open(os.path.join(self.root, cname,\n",
        "                                            img_id + '.jpg')).convert('RGB')\n",
        "                mask = Image.open(os.path.join(self.root, cname,\n",
        "                                            img_id + '.png')).convert('RGB')\n",
        "                img, mask = np.array(img), np.array(mask)[:, :, 0]\n",
        "                transformed = self.transform(image=img, mask=mask)\n",
        "                batch[i, j, :3, :, :] = transformed['image']\n",
        "                batch[i, j, 3:, :, :] = self.thresh_mask(transformed['mask'])\n",
        "\n",
        "        return batch\n",
        "\n",
        "    @staticmethod\n",
        "    def break_batch(batch, shots, ways, shuffle=True):\n",
        "        permute = torch.randperm(ways) if shuffle else torch.arange(ways)\n",
        "        train_images, train_masks = batch[:, :shots, permute, :3, :, :],\\\n",
        "            batch[:, :shots, permute, 3:, :, :]\n",
        "\n",
        "        test_images, test_masks = batch[:, shots:, :, :3, :, :],\\\n",
        "            batch[:, shots:, :, 3:, :, :]\n",
        "\n",
        "        return (train_images, train_masks), (test_images, test_masks)\n",
        "\n",
        "    def __getitem__(self, class_index):\n",
        "        classes = [\n",
        "                   self.classes[i] for i in range(class_index, (class_index + self.ways) %\n",
        "                                                  self.num_classes)\n",
        "                   ]\n",
        "        batch = self.make_batch(classes)\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_classes\n",
        "\n",
        "    def download(self, root, remove_zip=True):\n",
        "        filename = data_config['dataset_dir'] + '.zip'\n",
        "\n",
        "        if os.path.exists(root):\n",
        "            return\n",
        "\n",
        "        file_id = data_config['gdrive_file_id']\n",
        "\n",
        "        download_file_from_google_drive(file_id, filename)\n",
        "\n",
        "        with zipfile.ZipFile(filename, 'r') as f:\n",
        "            f.extractall()\n",
        "\n",
        "        if remove_zip:\n",
        "            os.remove(filename)\n",
        "\n",
        "        shutil.move(data_config['dataset_dir'], root)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE6UK1cAyuy-",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\"\"\"Helper functions for retrieving dataloaders\"\"\"\n",
        "\n",
        "\n",
        "def get_dataset(algo: str, meta_split='train'):\n",
        "    \"\"\"Retrieves dataset corresponding to parameters\n",
        "        defined in config module for the meta learning algorithm used.\n",
        "\n",
        "    Args:\n",
        "      algo (str): Meta learning algorithm from [maml, fomaml,\\\n",
        "       meta-sgd, reptile]\n",
        "      meta_split (str, optional): 'train' or 'test' split of the data.\\\n",
        "       Defaults to 'train'.\n",
        "\n",
        "    Returns:\n",
        "      Dataset: An instance of the Dataset class.\n",
        "    \"\"\"\n",
        "    transform = get_transforms()\n",
        "    data_root = data_config['data_root']\n",
        "    n_ways = data_config[algo]['n_ways']\n",
        "    train_shots = data_config[algo]['train_shots']\n",
        "    test_shots = data_config[algo]['test_shots']\n",
        "\n",
        "    dataset = FSSDataset(data_root, n_ways, train_shots, test_shots, meta_split,\n",
        "                       transform)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_dataloader(algo: str, meta_split='train'):\n",
        "    \"\"\"Retrieves a PyTorch dataloader. Parameters for the dataloader can be\n",
        "        found in the data_config module\n",
        "\n",
        "    Args:\n",
        "      algo (str): Meta learning algorithm from [maml, fomaml,\n",
        "       meta-sgd, reptile]\n",
        "      meta_split (str, optional): 'train' or 'test' split of the data.\n",
        "       Defaults to 'train'.\n",
        "\n",
        "    Returns:\n",
        "      DataLoader: An instance of DataLoader class.\n",
        "    \"\"\"\n",
        "    batch_size = data_config['batch_size']\n",
        "    num_workers = data_config['num_workers']\n",
        "    dataset = get_dataset(algo, meta_split)\n",
        "    shuffle = meta_split == 'train'\n",
        "    return DataLoader(dataset,\n",
        "                    batch_size,\n",
        "                    shuffle=shuffle,\n",
        "                    num_workers=num_workers)\n",
        "\n",
        "\n",
        "def split_batch(\n",
        "    batch: Tensor,\n",
        "    algo: str,\n",
        "    meta_split: str) -> Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]:\n",
        "    \"\"\"Splits a batch of data into image and mask pairs from the support\n",
        "        set and the query set.\n",
        "\n",
        "    Args:\n",
        "      batch (Tensor): A torch.Tensor returned by the dataloader\n",
        "      algo (str): Meta learning algorithm from [maml, fomaml,\n",
        "       meta-sgd, reptile]\n",
        "\n",
        "    Returns:\n",
        "      [[Tensor, Tensor], [Tensor, Tensor]: Four Tensors paired into two tuples.\n",
        "      (support_images, support_targets), (query_images, query_targets)\n",
        "    \"\"\"\n",
        "    n_ways = data_config[algo]['n_ways']\n",
        "    train_shots = data_config[algo]['train_shots']\n",
        "    shuffle = meta_split == 'train'\n",
        "    return FSSDataset.break_batch(batch, train_shots, n_ways, shuffle)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2qsiKzjy4FH",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "class DifferentiableAdamW(DifferentiableOptimizer):\n",
        "    r\"\"\"A differentiable version of the Adam optimizer.\n",
        "\n",
        "        This optimizer creates a gradient tape as it updates parameters.\"\"\"\n",
        "\n",
        "    def _update(self, grouped_grads: _GroupedGradsType, **kwargs) -> None:\n",
        "    \n",
        "        zipped = zip(self.param_groups, grouped_grads)\n",
        "        for group_idx, (group, grads) in enumerate(zipped):\n",
        "            amsgrad = group['amsgrad']\n",
        "            beta1, beta2 = group['betas']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p_idx, (p, g) in enumerate(zip(group['params'], grads)):\n",
        "\n",
        "                if g is None:\n",
        "                    continue\n",
        "\n",
        "                # Perform stepweight decay\n",
        "                if group['lr']==1.0:\n",
        "                    p = p * (1 - self.task_lr[p_idx] * weight_decay)\n",
        "                else:\n",
        "                    p = p * (1 - group['lr'] * weight_decay)\n",
        "\n",
        "                if g.is_sparse:\n",
        "                    raise RuntimeError('AdamW does not support sparse gradients')\n",
        "\n",
        "                state = self.state[group_idx][p_idx]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = _torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = _torch.zeros_like(p.data)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. mov. avg. of sq. grad. vals\n",
        "                        state['max_exp_avg_sq'] = _torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1**state['step']\n",
        "                bias_correction2 = 1 - beta2**state['step']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                state['exp_avg'] = exp_avg = (exp_avg * beta1) + (1 - beta1) * g\n",
        "                state['exp_avg_sq'] = exp_avg_sq = ((exp_avg_sq * beta2) +\n",
        "                                                    (1 - beta2) * g * g)\n",
        "\n",
        "                # Deal with stability issues\n",
        "                mask = exp_avg_sq == 0.\n",
        "                _maybe_mask(exp_avg_sq, mask)\n",
        "\n",
        "                if amsgrad:\n",
        "                    # Maintains the max of all 2nd moment running avg. till now\n",
        "                    state['max_exp_avg_sq'] = max_exp_avg_sq = _torch.max(\n",
        "                        max_exp_avg_sq, exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = _add(max_exp_avg_sq.sqrt() / _math.sqrt(bias_correction2), group['eps'])\n",
        "                else:\n",
        "                    denom = _add(exp_avg_sq.sqrt() / _math.sqrt(bias_correction2), group['eps'])\n",
        "\n",
        "                if group['lr']==1.0:\n",
        "                    step_size = (self.task_lr[p_idx]  / bias_correction1)\n",
        "                else:\n",
        "                    step_size = (group['lr']  / bias_correction1)\n",
        "\n",
        "                group['params'][p_idx] = _addcdiv(p, -step_size, exp_avg, denom)\n",
        "\n",
        "    def store_task_lr(self,task_lr):\n",
        "        self.task_lr = task_lr\n",
        "\n",
        "\n",
        "\n",
        "class InnerOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, algo='maml'):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        defaults = dict(lr=lr, algo=algo)\n",
        "        super(InnerOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "class DifferentiableInnerOptimizer(DifferentiableOptimizer):\n",
        "\n",
        "    def _update(self, grouped_grads: _GroupedGradsType, **kwargs) -> None:\n",
        "        \n",
        "        zipped = zip(self.param_groups, grouped_grads)\n",
        "        for group_idx, (group, grads) in enumerate(zipped):\n",
        "            algo = group['algo']\n",
        "\n",
        "            for p_idx, (p, g) in enumerate(zip(group['params'], grads)):\n",
        "                if g is None:\n",
        "                    continue\n",
        "\n",
        "                if algo == 'meta-sgd':\n",
        "                    group['params'][p_idx] = _add(p, -self.task_lr[p_idx], g)\n",
        "                else:\n",
        "                    group['params'][p_idx] = _add(p, -group['lr'], g)\n",
        "\n",
        "    def store_task_lr(self,task_lr):\n",
        "        self.task_lr = task_lr        \n",
        "    \n",
        "\n",
        "register_optim(InnerOptimizer, DifferentiableInnerOptimizer)\n",
        "\n",
        "register_optim(AdamW, DifferentiableAdamW)\n",
        "\n",
        "\"\"\"Utility functions for retrieving optimizers\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def get_optimizers(\n",
        "    module: Module,\n",
        "    algo: str) -> Any:\n",
        "    meta_lr = train_config[algo]['meta_lr']\n",
        "    learner_lr = train_config[algo]['learner_lr']\n",
        "    n_epochs = train_config['n_epochs']\n",
        "    if algo == 'meta-sgd':\n",
        "        meta_optimizer = optim.AdamW(list(module.parameters())+list(module.task_lr.values()), \n",
        "                                     meta_lr)\n",
        "        # learner_optimizer = InnerOptimizer(module.parameters(), learner_lr, algo)\n",
        "        learner_optimizer = optim.AdamW(module.parameters(), 1.0)\n",
        "    else:\n",
        "        meta_optimizer = optim.AdamW(module.parameters(), meta_lr)\n",
        "        learner_optimizer = optim.AdamW(module.parameters(), learner_lr)\n",
        "    \n",
        "    if algo in ['maml', 'fomaml', 'meta-sgd']:\n",
        "        patience = train_config[algo]['halve_lr_every']\n",
        "        lr_red = train_config[algo]['lr_reduction_factor']\n",
        "        metric_to_watch = train_config[algo]['metric_to_watch']\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(meta_optimizer, 'max',\n",
        "                                                        lr_red, patience)\n",
        "        return {\n",
        "            'meta_optimizer': meta_optimizer,\n",
        "            'learner_optimizer': learner_optimizer,\n",
        "            'scheduler': scheduler,\n",
        "            'monitor': metric_to_watch\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        final_meta_lr = train_config[algo]['final_meta_lr']\n",
        "        slope = (meta_lr - final_meta_lr) / n_epochs\n",
        "        scheduler = optim.lr_scheduler.LambdaLR(meta_optimizer, lambda epoch:\n",
        "                                                (epoch + 1) * slope)\n",
        "        return {\n",
        "            'meta_optimizer': meta_optimizer,\n",
        "            'learner_optimizer': learner_optimizer,\n",
        "            'scheduler': scheduler\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofTfvx0WzBa9",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\"\"\" Residual components of the network\"\"\"\n",
        "\n",
        "\n",
        "class Resblock(nn.Module):\n",
        "\n",
        "    def __init__(self, block_id: int):\n",
        "        super().__init__()\n",
        "        self.block_id = 'resblock%d' % block_id\n",
        "\n",
        "        self.add_module('conv1',\n",
        "                        nn.Conv2d(**model_config['resblocks'][self.block_id]['conv1']))\n",
        "        self.add_module('conv2',\n",
        "                        nn.Conv2d(**(model_config['resblocks'][self.block_id]['conv2'])))\n",
        "        self.add_module('reducer', nn.Conv2d(**model_config['reducer'][self.block_id]))\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        x_init = x\n",
        "\n",
        "        for i, block in enumerate(self.children()):\n",
        "            if i == 2:\n",
        "                break\n",
        "            x = block(x)\n",
        "\n",
        "        return x + block(x_init)\n",
        "\n",
        "\"\"\"Implements the model described in arxiv.2008.00247\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"MetaDRN architectured described in arxiv.2008.00247\"\"\"\n",
        "\n",
        "class MetaDRN(nn.Module):\n",
        "    def __init__(self, algo='maml', init_inner_learner_lr=1e-3):\n",
        "        super().__init__()\n",
        "        # Definet the network\n",
        "        self.head = nn.Sequential()\n",
        "        self.head.add_module(\"conv1\", nn.Conv2d(**model_config[\"head\"][\"conv1\"]))\n",
        "        self.head.add_module(\"bn1\", nn.BatchNorm2d(**model_config[\"head\"][\"bn1\"]))\n",
        "        self.head.add_module(\"lr1\", nn.LeakyReLU())\n",
        "        self.head.add_module(\"conv2\", nn.Conv2d(**model_config[\"head\"][\"conv2\"]))\n",
        "        self.head.add_module(\"bn2\", nn.BatchNorm2d(**model_config[\"head\"][\"bn2\"]))\n",
        "        self.head.add_module(\"lr2\", nn.LeakyReLU())\n",
        "\n",
        "        self.resblock1 = nn.Sequential()\n",
        "        self.resblock1.add_module(\"resblock1\", Resblock(1))\n",
        "        \n",
        "        self.resblock2 = nn.Sequential()\n",
        "        self.resblock2.add_module(\"resblock2\", Resblock(2))\n",
        "        \n",
        "        self.resblock3 = nn.Sequential()\n",
        "        self.resblock3.add_module(\"resblock3\", Resblock(3))\n",
        "\n",
        "        self.degrid = nn.Sequential()\n",
        "        self.degrid.add_module(\"conv1\", nn.Conv2d(**model_config[\"degrid\"][\"conv1\"]))\n",
        "        self.degrid.add_module(\"conv2\", nn.Conv2d(**model_config[\"degrid\"][\"conv2\"]))\n",
        "\n",
        "        self.upsample = nn.Sequential(\n",
        "            OrderedDict([(\"conv1\", nn.Conv2d(**model_config[\"upsample\"][\"conv\"])),\n",
        "                         (\"pixel_shuffle\",\n",
        "                          nn.PixelShuffle(**model_config[\"upsample\"][\"pixel_shuffle\"]))]))\n",
        "        if algo == \"meta-sgd\":\n",
        "            self.task_lr = OrderedDict()\n",
        "            self.init_inner_learner_lr = init_inner_learner_lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.upsample(self.degrid(self.resblock3(self.resblock2(self.resblock1(self.head(x))))))\n",
        "\n",
        "    def define_task_lr_params(self):\n",
        "        for key, val in self.named_parameters():\n",
        "            self.task_lr[key] = nn.Parameter(\n",
        "                self.init_inner_learner_lr * torch.ones_like(val, requires_grad=True).cuda())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2biIJFg8bN-t"
      },
      "source": [
        "#@title\n",
        "# %matplotlib inline\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "            # The normalize code -> t.sub_(m).div_(s)\n",
        "        return tensor\n",
        "\n",
        "unorm = UnNormalize(mean=data_config['normalize_mean'], std=data_config['normalize_std'])\n",
        "\n",
        "activation = {}\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "def max_meanFeature(x):\n",
        "    return x[torch.arange(x.size(0)),\n",
        "             torch.argmax(torch.mean(x, dim=(2,3)), dim=1),\n",
        "             :,\n",
        "             :].view(-1,1,x.size(2),x.size(3))\n",
        "\n",
        "def get_matplotFig(spt_x, spt_y, qry_x, qry_y):\n",
        "    # plt.ioff()\n",
        "    nrows = spt_x.shape[0]\n",
        "    ncols = 2\n",
        "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(2*ncols,2*nrows), gridspec_kw = {'wspace':0, 'hspace':0})\n",
        "    xlabels = [\"Support\",\"MAML\"]\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            if j==0:\n",
        "                img = spt_x[i,:,:,:]\n",
        "                musk = spt_y[i,:,:,:]\n",
        "            else:\n",
        "                img = qry_x[i,:,:,:]\n",
        "                musk = qry_y[i,:,:,:]\n",
        "            img = img.cpu()\n",
        "            img = unorm(img)\n",
        "            img = img.permute(1,2,0)\n",
        "            musk = musk.cpu()\n",
        "            musk = musk.permute(1,2,0)\n",
        "            musk_with_alpha = np.zeros(shape=(musk.shape[0], musk.shape[1], 4),dtype=np.float)\n",
        "            musk_with_alpha[:,:,1-j] = musk[:,:,0]\n",
        "            musk_with_alpha[:,:,3] = musk[:,:,0]/2.5\n",
        "\n",
        "            axs[i][j].imshow(img)\n",
        "            axs[i][j].imshow(musk_with_alpha)\n",
        "            # axs[i][j].set_axis_off()\n",
        "            axs[i][j].set_xticks([])\n",
        "            axs[i][j].set_yticks([])\n",
        "            if i == nrows-1:\n",
        "                axs[i][j].set_xlabel(xlabels[j],size=16)\n",
        "            \n",
        "    return fig\n",
        "\n",
        "def get_activationFig(spt_x, spt_y, qry_x, out_y, activation):\n",
        "    nrows = spt_x.size(0)\n",
        "    ncols = 8\n",
        "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(2*ncols,2*nrows), gridspec_kw = {'wspace':0, 'hspace':0})\n",
        "    keys = list(activation.keys())\n",
        "    activation = {key:max_meanFeature(activation[key]) for key in keys}\n",
        "    xlabels = [\"Support\",\"Image\",\"Head\",\"ResBlock-1\",\"ResBlock-2\",\"ResBlock-3\",\"Degrid\",\"Final output\"]\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            if j==0:\n",
        "                img = spt_x[i,:,:,:]\n",
        "                musk = spt_y[i,:,:,:]\n",
        "                img = unorm(img)\n",
        "                musk = musk.cpu()\n",
        "            elif j==1:\n",
        "                img = qry_x[i,:,:,:]\n",
        "                img = unorm(img)\n",
        "            elif j==ncols-1:\n",
        "                img = out_y[i,:,:,:]\n",
        "            else:\n",
        "                img = activation[keys[j-2]]\n",
        "                img = img[i,:,:,:]\n",
        "                img = (img - img.mean())/img.std()\n",
        "            \n",
        "            img = img.permute(1,2,0)\n",
        "            img = img.cpu()\n",
        "            if 0<=j<=1:\n",
        "                axs[i][j].imshow(img)\n",
        "            else:\n",
        "                axs[i][j].imshow(img.squeeze(),cmap='gray')\n",
        "            if j == 0:\n",
        "                musk = musk.permute(1,2,0)\n",
        "                musk_with_alpha = np.zeros(shape=(musk.shape[0], musk.shape[1], 4),dtype=np.float)\n",
        "                musk_with_alpha[:,:,1-j] = musk[:,:,0]\n",
        "                musk_with_alpha[:,:,3] = musk[:,:,0]/2.5\n",
        "                axs[i][j].imshow(musk_with_alpha)\n",
        "            # axs[i][j].set_axis_off()\n",
        "            axs[i][j].set_xticks([])\n",
        "            axs[i][j].set_yticks([])\n",
        "            if i == nrows-1:\n",
        "                axs[i][j].set_xlabel(xlabels[j],size=16)\n",
        "            \n",
        "    return fig\n",
        "    \n",
        "def add_hook_to_Model(net):\n",
        "    net.head.register_forward_hook(get_activation('head'))\n",
        "    net.resblock1.register_forward_hook(get_activation('resblock1'))\n",
        "    net.resblock2.register_forward_hook(get_activation('resblock2'))\n",
        "    net.resblock3.register_forward_hook(get_activation('resblock3'))\n",
        "    net.degrid.register_forward_hook(get_activation('degrid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIZz51vwjJJm",
        "outputId": "a4d5a648-54d0-4e32-f19a-ebda733f42e3"
      },
      "source": [
        "#@title\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmj5-j33zZV_"
      },
      "source": [
        "#@title\n",
        "# seeding\n",
        "seed = 1971\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# hyperparams\n",
        "algo = 'meta-sgd'\n",
        "n_epochs = train_config['n_epochs']\n",
        "learner_lr = train_config[algo]['learner_lr']\n",
        "\n",
        "# get dataloader\n",
        "train_loader = get_dataloader(algo, 'train')\n",
        "val_loader = get_dataloader(algo, 'val')\n",
        "test_loader = get_dataloader(algo, 'test')\n",
        "# get model\n",
        "if True:\n",
        "    net = MetaDRN(algo=algo)\n",
        "    if algo == 'meta-sgd':\n",
        "        net.define_task_lr_params()\n",
        "else:\n",
        "    net = torch.load(\"/content/drive/MyDrive/Colab Notebooks/models/meta_drn_maml_9.pt\")\n",
        "    net.eval()\n",
        "net.cuda()\n",
        "optimizers = get_optimizers(net, algo)\n",
        "learner_optim = optimizers['learner_optimizer']\n",
        "meta_optim = optimizers['meta_optimizer']\n",
        "lr_scheduler = optimizers['scheduler']\n",
        "# metric_to_watch = optimizers['monitor']\n",
        "# log data for tensorboard visualization\n",
        "# logs = vis_config['tensorboard']['logdir']+algo\n",
        "# writer = SummaryWriter(logs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPECOqHEzq9O",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def train(net, loader, epoch=0, writer=None):\n",
        "    net.train()\n",
        "    qry_losses = []\n",
        "    qry_ious = []\n",
        "    pbar = tqdm(loader)\n",
        "    \n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        # (train_x, train_y), (test_x, test_y) = split_batch(batch, algo, 'train')\n",
        "        (spt_x, spt_y), (qry_x, qry_y ) = split_batch(batch, algo, 'train')\n",
        "        # tasks = train_x.size(0)\n",
        "        \n",
        "        spt_x, spt_y = spt_x.view(-1, *spt_x.shape[3:]).cuda(), spt_y.view(\n",
        "           -1, *spt_y.shape[3:]).cuda()\n",
        "        qry_x, qry_y = qry_x.view(-1, *qry_x.shape[3:]).cuda(), qry_y.view(\n",
        "           -1, *qry_y.shape[3:]).cuda()\n",
        "\n",
        "        # weights_before = deepcopy(net.state_dict())\n",
        "        \n",
        "        \n",
        "        \n",
        "        # meta_losses = []\n",
        "        # for tsk in range(tasks):\n",
        "        #     spt_x, spt_y = train_x[tsk,:,:,:,:,:], train_y[tsk,:,:,:,:,:]\n",
        "        #     qry_x, qry_y = test_x[tsk,:,:,:,:,:], test_y[tsk,:,:,:,:,:]\n",
        "\n",
        "        #     spt_x, spt_y = spt_x.view(-1, *spt_x.shape[2:]).cuda(), spt_y.view(\n",
        "        #     -1, *spt_y.shape[2:]).cuda()\n",
        "        #     qry_x, qry_y = qry_x.view(-1, *qry_x.shape[2:]).cuda(), qry_y.view(\n",
        "        #     -1, *qry_y.shape[2:]).cuda()\n",
        "        \n",
        "        if algo in ['maml', 'fomaml', 'meta-sgd']:\n",
        "            meta_optim.zero_grad()\n",
        "            with higher.innerloop_ctx(net, learner_optim, \n",
        "                                        copy_initial_weights=False, \n",
        "                                        track_higher_grads=(algo in ['maml','meta-sgd'])) as (fnet, diffoptim):\n",
        "                for i in range(train_config[algo]['train_steps']):\n",
        "                    pred = fnet(spt_x)\n",
        "                    loss = F.cross_entropy(pred, spt_y.squeeze().long())\n",
        "                    if i==0 and algo == 'meta-sgd':\n",
        "                        diffoptim.store_task_lr(list(net.task_lr.values()))\n",
        "                    diffoptim.step(loss)\n",
        "\n",
        "                qry_logits = fnet(qry_x)\n",
        "                qry_loss = F.cross_entropy(qry_logits, qry_y.squeeze().long())\n",
        "                \n",
        "                qry_losses.append(qry_loss.detach())\n",
        "                with torch.no_grad():\n",
        "                    qry_iou = iou(torch.argmax(qry_logits, dim=1), qry_y.squeeze().long())\n",
        "                    qry_ious.append(qry_iou)\n",
        "                qry_loss.backward()\n",
        "            meta_optim.step()\n",
        "            lr_scheduler.step(qry_losses[-1])\n",
        "            pbar.set_description(\"Epoch: %d, training Loss: %.2f, mIoU: %.2f, time: %s\" %\n",
        "                                (epoch, qry_losses[-1], qry_ious[-1], time.strftime('%X')))\n",
        "        elif algo == 'reptile':\n",
        "            weights_before = deepcopy(net.state_dict())\n",
        "            for _ in range(train_config[algo]['train_steps']):\n",
        "                    pred = net(spt_x)\n",
        "                    loss = F.cross_entropy(pred, spt_y.squeeze().long())\n",
        "                    net.zero_grad()\n",
        "                    loss.backward()\n",
        "                    learner_optim.step()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                qry_logits = net(qry_x)\n",
        "                qry_loss = F.cross_entropy(qry_logits, qry_y.squeeze().long())\n",
        "                qry_losses.append(qry_loss.detach())\n",
        "                qry_iou = iou(torch.argmax(qry_logits, dim=1), qry_y.squeeze().long())\n",
        "                qry_ious.append(qry_iou)\n",
        "                pbar.set_description(\"Epoch: %d, training Loss: %.2f, mIoU: %.2f, time: %s\" %\n",
        "                                (epoch, qry_losses[-1], qry_iou, time.strftime('%X')))\n",
        "            net.zero_grad()\n",
        "            weights_after = deepcopy(net.state_dict())\n",
        "            net.load_state_dict(weights_before)\n",
        "            for name, param in net.named_parameters():\n",
        "                param.grad.data = weights_before[name].data - weights_after[name].data\n",
        "            meta_optim.step()\n",
        "                \n",
        "    #         elif algo == 'fomal':\n",
        "    #             for _ in range(train_config[algo]['train_steps']):\n",
        "    #                 last_backup = deepcopy(net.state_dict())\n",
        "    #                 pred = net(spt_x)\n",
        "    #                 loss = F.cross_entropy(pred, spt_y.squeeze().long())\n",
        "    #                 net.zero_grad()\n",
        "    #                 loss.backward()\n",
        "    #                 learner_optim.step()\n",
        "                \n",
        "    #             with torch.no_grad():\n",
        "    #                 qry_logits = net(qry_x)\n",
        "    #                 qry_loss = F.cross_entropy(qry_logits, qry_y.squeeze().long())\n",
        "    #                 qry_losses.append(qry_loss.detach())\n",
        "    #                 qry_iou = iou(torch.argmax(qry_logits, dim=1), qry_y.squeeze().long())\n",
        "    #                 qry_ious.append(qry_iou)\n",
        "    #                 pbar.set_description(\"Epoch: %d, training Loss: %.2f, mIoU: %.2f, time: %s\" %\n",
        "    #                               (epoch, qry_losses[-1], qry_iou, time.strftime('%X')))\n",
        "\n",
        "    #         elif algo == 'reptile':\n",
        "    #             for _ in range(train_config[algo]['train_steps']):\n",
        "    #                 pred = net(spt_x)\n",
        "    #                 loss = F.cross_entropy(pred, spt_y.squeeze().long())\n",
        "    #                 net.zero_grad()\n",
        "    #                 loss.backward()\n",
        "    #                 learner_optim.step()\n",
        "    #             with torch.no_grad():\n",
        "    #                 qry_logits = net(qry_x)\n",
        "    #                 qry_loss = F.cross_entropy(qry_logits, qry_y.squeeze().long())\n",
        "    #                 qry_losses.append(qry_loss.detach())\n",
        "    #                 qry_iou = iou(torch.argmax(qry_logits, dim=1), qry_y.squeeze().long())\n",
        "    #                 qry_ious.append(qry_iou)\n",
        "    #                 pbar.set_description(\"Epoch: %d, training Loss: %.2f, mIoU: %.2f, time: %s\" %\n",
        "    #                               (epoch, qry_losses[-1], qry_iou, time.strftime('%X')))\n",
        "        \n",
        "    #     if algo =='maml':\n",
        "    #         pass\n",
        "    #         # meta_loss = sum(meta_losses)/len(meta_losses)\n",
        "    #         # meta_loss.backward()\n",
        "            \n",
        "    #     elif algo == 'reptile':\n",
        "    #         net.zero_grad()\n",
        "    #         weights_after = deepcopy(net.state_dict())\n",
        "    #         net.load_state_dict(weights_before)\n",
        "    #         names = []\n",
        "    #         for name in weights_before:\n",
        "    #             if \"bias\" in name or \"weight\" in name:\n",
        "    #                 names.append(name)\n",
        "\n",
        "    #         for param,name in zip(net.parameters(),names):\n",
        "    #             param.grad.data = weights_before[name].data - weights_after[name].data\n",
        "    #         meta_optim.step()\n",
        "            \n",
        "    if algo == 'reptile':\n",
        "        lr_scheduler.step()\n",
        "    \n",
        "\n",
        "    qry_loss_epoch = sum(qry_losses) / len(qry_losses)\n",
        "    qry_iou_epoch= sum(qry_ious) / len(qry_ious)\n",
        "    print(\"loss: {} iou: {}\".format(qry_loss_epoch, qry_iou_epoch))\n",
        "    if writer is not None:\n",
        "        writer.add_scalar('training loss', qry_loss_epoch, epoch)\n",
        "        writer.add_scalar('training mIoU', qry_iou_epoch, epoch)\n",
        "            \n",
        "        # pbar.set_description(\"Epoch: %d, Training Loss: %.2f, mIoU: %.2f, time: %s\" %\n",
        "                            #  (epoch, qry_loss, qry_iou, time.strftime('%X')))\n",
        "\n",
        "        #return {'loss': qry_loss, 'acc': qry_iou, 'time': time.time()}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9-XnF1NRXSS"
      },
      "source": [
        "# !kill 3071\n",
        "# %tensorboard --logdir \"/content/drive/MyDrive/Colab Notebooks/logsmaml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qB0ArZCjWdX"
      },
      "source": [
        "## train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vLk45Zl0Gfc"
      },
      "source": [
        "#@title\n",
        "if __name__ == '__main__':\n",
        "    for e in range(0,10):#n_epochs):\n",
        "        train(net, train_loader, epoch=e, writer=None)\n",
        "        if e%10==9:\n",
        "            torch.save(net,\"/content/drive/MyDrive/Colab Notebooks/models/meta_drn_\"+algo+'_'+str(e)+'.pt')\n",
        "        print(\"completed epoch {}\".format(e))\n",
        "        # validate(net, val_loader, epoch=e, writer=writer)\n",
        "        # test(net, test_loader, epoch=e, writer=writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkcfpvrOZNBp"
      },
      "source": [
        "# validate(net, val_loader, epoch=0, writer=writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT6yRKTOZO4l",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import math\n",
        "import random\n",
        "import torch # v0.4.1\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib as mpl\n",
        "# mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def net(x, params):\n",
        "    x = F.linear(x, params[0], params[1])\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = F.linear(x, params[2], params[3])\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = F.linear(x, params[4], params[5])\n",
        "    return x\n",
        "\n",
        "params = [\n",
        "    torch.Tensor(32, 1).uniform_(-1., 1.).requires_grad_(),\n",
        "    torch.Tensor(32).zero_().requires_grad_(),\n",
        "\n",
        "    torch.Tensor(32, 32).uniform_(-1./math.sqrt(32), 1./math.sqrt(32)).requires_grad_(),\n",
        "    torch.Tensor(32).zero_().requires_grad_(),\n",
        "\n",
        "    torch.Tensor(1, 32).uniform_(-1./math.sqrt(32), 1./math.sqrt(32)).requires_grad_(),\n",
        "    torch.Tensor(1).zero_().requires_grad_(),\n",
        "]\n",
        "\n",
        "opt = torch.optim.SGD(params, lr=1e-2)\n",
        "n_inner_loop = 5\n",
        "alpha = 3e-2\n",
        "\n",
        "for it in range(1):\n",
        "    b = 0 if random.choice([True, False]) else math.pi\n",
        "\n",
        "    x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "    y = torch.sin(x + b)\n",
        "\n",
        "    v_x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "    v_y = torch.sin(v_x + b)\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    new_params = params\n",
        "    print(new_params)\n",
        "    for k in range(n_inner_loop):\n",
        "        f = net(x, new_params)\n",
        "        loss = F.l1_loss(f, y)\n",
        "\n",
        "        # create_graph=True because computing grads here is part of the forward pass.\n",
        "        # We want to differentiate through the SGD update steps and get higher order\n",
        "        # derivatives in the backward pass.\n",
        "        grads = torch.autograd.grad(loss, new_params, create_graph=True)\n",
        "        new_params = [(new_params[i] - alpha*grads[i]) for i in range(len(params))]\n",
        "\n",
        "        if it % 1000 == 0: \n",
        "            print('Iteration %d -- Inner loop %d -- Loss: %.4f' % (it, k, loss))\n",
        "\n",
        "    v_f = net(v_x, new_params)\n",
        "    loss2 = F.l1_loss(v_f, v_y)\n",
        "    loss2.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "    if it % 1000 == 0: \n",
        "        print('Iteration %d -- Outer Loss: %.4f' % (it, loss2))\n",
        "\n",
        "t_b = math.pi #0\n",
        "\n",
        "t_x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "t_y = torch.sin(t_x + t_b)\n",
        "\n",
        "opt.zero_grad()\n",
        "\n",
        "t_params = params\n",
        "for k in range(n_inner_loop):\n",
        "    t_f = net(t_x, t_params)\n",
        "    t_loss = F.l1_loss(t_f, t_y)\n",
        "\n",
        "    grads = torch.autograd.grad(t_loss, t_params, create_graph=True)\n",
        "    t_params = [(t_params[i] - alpha*grads[i]) for i in range(len(params))]\n",
        "\n",
        "\n",
        "test_x = torch.arange(-2*math.pi, 2*math.pi, step=0.01).unsqueeze(1)\n",
        "test_y = torch.sin(test_x + t_b)\n",
        "\n",
        "test_f = net(test_x, t_params)\n",
        "\n",
        "plt.plot(test_x.data.numpy(), test_y.data.numpy(), label='sin(x)')\n",
        "plt.plot(test_x.data.numpy(), test_f.data.numpy(), label='net(x)')\n",
        "plt.plot(t_x.data.numpy(), t_y.data.numpy(), 'o', label='Examples')\n",
        "plt.legend()\n",
        "# plt.savefig('maml-sine.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T90ZR13s4kYc",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, autograd as ag\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "seed = 0\n",
        "plot = True\n",
        "innerstepsize = 0.02 # stepsize in inner SGD\n",
        "innerepochs = 1 # number of epochs of each inner SGD\n",
        "outerstepsize0 = 0.1 # stepsize of outer optimization, i.e., meta-optimization\n",
        "niterations = 30000 # number of outer updates; each iteration we sample one task and update on it\n",
        "\n",
        "rng = np.random.RandomState(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Define task distribution\n",
        "x_all = np.linspace(-5, 5, 50)[:,None] # All of the x points\n",
        "ntrain = 10 # Size of training minibatches\n",
        "def gen_task():\n",
        "    \"Generate classification problem\"\n",
        "    phase = rng.uniform(low=0, high=2*np.pi)\n",
        "    ampl = rng.uniform(0.1, 5)\n",
        "    f_randomsine = lambda x : np.sin(x + phase) * ampl\n",
        "    return f_randomsine\n",
        "\n",
        "# Define model. Reptile paper uses ReLU, but Tanh gives slightly better results\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 64),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64, 1),\n",
        ")\n",
        "\n",
        "def totorch(x):\n",
        "    return ag.Variable(torch.Tensor(x))\n",
        "\n",
        "def train_on_batch(x, y):\n",
        "    x = totorch(x)\n",
        "    y = totorch(y)\n",
        "    model.zero_grad()\n",
        "    ypred = model(x)\n",
        "    loss = (ypred - y).pow(2).mean()\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "        param.data -= innerstepsize * param.grad.data\n",
        "\n",
        "def predict(x):\n",
        "    x = totorch(x)\n",
        "    return model(x).data.numpy()\n",
        "\n",
        "# Choose a fixed task and minibatch for visualization\n",
        "f_plot = gen_task()\n",
        "xtrain_plot = x_all[rng.choice(len(x_all), size=ntrain)]\n",
        "\n",
        "# Reptile training loop\n",
        "for iteration in range(niterations):\n",
        "    weights_before = deepcopy(model.state_dict())\n",
        "    # Generate task\n",
        "    f = gen_task()\n",
        "    y_all = f(x_all)\n",
        "    # Do SGD on this task\n",
        "    inds = rng.permutation(len(x_all))\n",
        "    for _ in range(innerepochs):\n",
        "        for start in range(0, len(x_all), ntrain):\n",
        "            mbinds = inds[start:start+ntrain]\n",
        "            train_on_batch(x_all[mbinds], y_all[mbinds])\n",
        "    # Interpolate between current weights and trained weights from this task\n",
        "    # I.e. (weights_before - weights_after) is the meta-gradient\n",
        "    weights_after = model.state_dict()\n",
        "    outerstepsize = outerstepsize0 * (1 - iteration / niterations) # linear schedule\n",
        "    model.load_state_dict({name : \n",
        "        weights_before[name] + (weights_after[name] - weights_before[name]) * outerstepsize \n",
        "        for name in weights_before})\n",
        "\n",
        "    # Periodically plot the results on a particular task and minibatch\n",
        "    if plot and iteration==0 or (iteration+1) % 10000 == 0:\n",
        "        plt.cla()\n",
        "        f = f_plot\n",
        "        weights_before = deepcopy(model.state_dict()) # save snapshot before evaluation\n",
        "        plt.plot(x_all, predict(x_all), label=\"pred after 0\", color=(0,0,1))\n",
        "        for inneriter in range(32):\n",
        "            train_on_batch(xtrain_plot, f(xtrain_plot))\n",
        "            if (inneriter+1) % 8 == 0:\n",
        "                frac = (inneriter+1) / 32\n",
        "                plt.plot(x_all, predict(x_all), label=\"pred after %i\"%(inneriter+1), color=(frac, 0, 1-frac))\n",
        "        plt.plot(x_all, f(x_all), label=\"true\", color=(0,1,0))\n",
        "        lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
        "        plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
        "        plt.ylim(-4,4)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.pause(0.01)\n",
        "        model.load_state_dict(weights_before) # restore from snapshot\n",
        "        print(f\"-----------------------------\")\n",
        "        print(f\"iteration               {iteration+1}\")\n",
        "        print(f\"loss on plotted curve   {lossval:.3f}\") # would be better to average loss over a set of examples, but this is optimized for brevity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIpC5IYQA1N4",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#reptile\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, autograd as ag\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "seed = 0\n",
        "plot = True\n",
        "innerstepsize = 0.02 # stepsize in inner SGD\n",
        "innerepochs = 1 # number of epochs of each inner SGD\n",
        "outerstepsize0 = 0.1 # stepsize of outer optimization, i.e., meta-optimization\n",
        "niterations = 30000 # number of outer updates; each iteration we sample one task and update on it\n",
        "\n",
        "rng = np.random.RandomState(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Define task distribution\n",
        "x_all = np.linspace(-5, 5, 50)[:,None] # All of the x points\n",
        "ntrain = 10 # Size of training minibatches\n",
        "def gen_task():\n",
        "    \"Generate classification problem\"\n",
        "    phase = rng.uniform(low=0, high=2*np.pi)\n",
        "    ampl = rng.uniform(0.1, 5)\n",
        "    f_randomsine = lambda x : np.sin(x + phase) * ampl\n",
        "    return f_randomsine\n",
        "\n",
        "# Define model. Reptile paper uses ReLU, but Tanh gives slightly better results\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 64),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64, 1),\n",
        ")\n",
        "\n",
        "def totorch(x):\n",
        "    # return ag.Variable(torch.Tensor(x))\n",
        "    return torch.Tensor(x)\n",
        "\n",
        "def train_on_batch(x, y):\n",
        "    x = totorch(x)\n",
        "    y = totorch(y)\n",
        "    model.zero_grad()\n",
        "    ypred = model(x)\n",
        "    loss = (ypred - y).pow(2).mean()\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "        param.data -= innerstepsize * param.grad.data\n",
        "\n",
        "def predict(x):\n",
        "    x = totorch(x)\n",
        "    return model(x).data.numpy()\n",
        "\n",
        "# Choose a fixed task and minibatch for visualization\n",
        "f_plot = gen_task()\n",
        "xtrain_plot = x_all[rng.choice(len(x_all), size=ntrain)]\n",
        "\n",
        "meta_optimizer = optim.SGD(model.parameters(), outerstepsize0)\n",
        "learner_optimizer = optim.SGD(model.parameters(), innerstepsize)\n",
        "\n",
        "# Reptile training loop\n",
        "for iteration in range(niterations):\n",
        "    weights_before = deepcopy(model.state_dict())\n",
        "    # print(weights_before)\n",
        "    # Generate task\n",
        "    f = gen_task()\n",
        "    y_all = f(x_all)\n",
        "    # Do SGD on this task\n",
        "    inds = rng.permutation(len(x_all))\n",
        "    for _ in range(innerepochs):\n",
        "        for start in range(0, len(x_all), ntrain):\n",
        "            mbinds = inds[start:start+ntrain]\n",
        "            x_qry = totorch(x_all[mbinds])\n",
        "            y_qry = totorch(y_all[mbinds])\n",
        "            pred = model(x_qry)\n",
        "            loss = F.mse_loss(pred,y_qry)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            learner_optimizer.step()\n",
        "                \n",
        "    model.zero_grad()\n",
        "    weights_after = deepcopy(model.state_dict())\n",
        "    model.load_state_dict(weights_before)\n",
        "    for param,name in zip(model.parameters(), weights_after):\n",
        "        param.grad = weights_before[name] - weights_after[name]\n",
        "    # for param in model.parameters():\n",
        "    #     print(param.grad)\n",
        "    meta_optimizer.step() \n",
        "        # for param in model.parameters():\n",
        "        #     print(param.data)\n",
        "\n",
        "    # Interpolate between current weights and trained weights from this task\n",
        "    # I.e. (weights_before - weights_after) is the meta-gradient\n",
        "    # weights_after = model.state_dict()\n",
        "    # outerstepsize = outerstepsize0 * (1 - iteration / niterations) # linear schedule\n",
        "    # model.load_state_dict({name : \n",
        "    #     weights_before[name] + (weights_after[name] - weights_before[name]) * outerstepsize \n",
        "    #     for name in weights_before})\n",
        "\n",
        "    # Periodically plot the results on a particular task and minibatch\n",
        "    if plot and iteration==0 or (iteration+1) % 10000 == 0:\n",
        "        plt.cla()\n",
        "        f = f_plot\n",
        "        weights_before = deepcopy(model.state_dict()) # save snapshot before evaluation\n",
        "        plt.plot(x_all, predict(x_all), label=\"pred after 0\", color=(0,0,1))\n",
        "        for inneriter in range(32):\n",
        "\n",
        "            # train_on_batch(xtrain_plot, f(xtrain_plot))\n",
        "\n",
        "            x_qry = totorch(xtrain_plot)\n",
        "            y_qry = totorch(f(xtrain_plot))\n",
        "            pred = model(x_qry)\n",
        "            loss = F.mse_loss(pred,y_qry)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            learner_optimizer.step()\n",
        "\n",
        "            if (inneriter+1) % 8 == 0:\n",
        "                frac = (inneriter+1) / 32\n",
        "                plt.plot(x_all, predict(x_all), label=\"pred after %i\"%(inneriter+1), color=(frac, 0, 1-frac))\n",
        "        plt.plot(x_all, f(x_all), label=\"true\", color=(0,1,0))\n",
        "        lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
        "        plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
        "        plt.ylim(-4,4)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.pause(0.01)\n",
        "        model.load_state_dict(weights_before) # restore from snapshot\n",
        "        print(f\"-----------------------------\")\n",
        "        print(f\"iteration               {iteration+1}\")\n",
        "        print(f\"loss on plotted curve   {lossval:.3f}\") # would be better to average loss over a set of examples, but this is optimized for brevity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3styCp6Lyfi",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#maml with heigher version 1\n",
        "import math\n",
        "import random\n",
        "import torch # v0.4.1\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib as mpl\n",
        "# mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import higher\n",
        "from torch import Tensor\n",
        "\n",
        "def net1(x, params):\n",
        "    x = F.linear(x, params[0], params[1])\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = F.linear(x, params[2], params[3])\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = F.linear(x, params[4], params[5])\n",
        "    return x\n",
        "\n",
        "# net2 = nn.Sequential(\n",
        "#     nn.Linear(1, 2),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(2, 2),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(2, 1),\n",
        "# )\n",
        "\n",
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor, params=None):\n",
        "        if params is not None:\n",
        "            x = F.linear(x, params[0], params[1])\n",
        "            x = F.relu(x)\n",
        "\n",
        "            x = F.linear(x, params[2], params[3])\n",
        "            x = F.relu(x)\n",
        "\n",
        "            x = F.linear(x, params[4], params[5])\n",
        "            return x\n",
        "\n",
        "        return self.net(x)\n",
        "\n",
        "net2 = myNet()\n",
        "\n",
        "params = [\n",
        "    torch.Tensor(2, 1).uniform_(-1., 1.).requires_grad_(),\n",
        "    torch.Tensor(2).zero_().requires_grad_(),\n",
        "\n",
        "    torch.Tensor(2, 2).uniform_(-1./math.sqrt(2), 1./math.sqrt(2)).requires_grad_(),\n",
        "    torch.Tensor(2).zero_().requires_grad_(),\n",
        "\n",
        "    torch.Tensor(1, 2).uniform_(-1./math.sqrt(2), 1./math.sqrt(2)).requires_grad_(),\n",
        "    torch.Tensor(1).zero_().requires_grad_()\n",
        "]\n",
        "\n",
        "for i,param in enumerate(net2.parameters()):\n",
        "    param.data = params[i].data\n",
        "    \n",
        "net2.zero_grad()\n",
        "for i,param in enumerate(net2.parameters()):\n",
        "    print(param.grad)\n",
        "    print('****************')\n",
        "    print(params[i])\n",
        "    print('#################')\n",
        "\n",
        "opt1 = torch.optim.SGD(params, lr=1e-2)\n",
        "opt2 = torch.optim.SGD(net2.parameters(), lr=1e-2)\n",
        "n_inner_loop = 5\n",
        "alpha = 3e-2\n",
        "opt3 = torch.optim.SGD(net2.parameters(), lr=alpha)\n",
        "\n",
        "for it in range(10000):\n",
        "    b = 0 if random.choice([True, False]) else math.pi\n",
        "\n",
        "    x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "    y = torch.sin(x + b)\n",
        "\n",
        "    v_x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "    v_y = torch.sin(v_x + b)\n",
        "\n",
        "\n",
        "    \n",
        "    opt1.zero_grad\n",
        "    metalosses = []\n",
        "    new_params = params\n",
        "    new_param2 = [param for param in net2.parameters()]\n",
        "    with higher.innerloop_ctx(net2,opt3,copy_initial_weights=False) as (fnet,diffoptim):\n",
        "        for k in range(n_inner_loop):\n",
        "            f2 = fnet(x)\n",
        "            loss2 = F.l1_loss(f2, y)\n",
        "            diffoptim.step(loss2)\n",
        "\n",
        "            f1 = net1(x, new_params)\n",
        "            loss1 = F.l1_loss(f1, y)\n",
        "\n",
        "            # create_graph=True because computing grads here is part of the forward pass.\n",
        "            # We want to differentiate through the SGD update steps and get higher order\n",
        "            # derivatives in the backward pass.\n",
        "            grads = torch.autograd.grad(loss1, new_params, create_graph=True)\n",
        "            new_params = [(new_params[i] - alpha*grads[i]) for i in range(len(params))]\n",
        "\n",
        "        if it % 1000 == 0: \n",
        "            print('Iteration %d -- Inner loop %d -- Loss: %.4f  Loss1: %.4f' % (it, k, loss2, loss1))\n",
        "        metalosses.append(F.l1_loss(fnet(v_x), v_y))\n",
        "    opt2.zero_grad()\n",
        "    meta_loss = sum(metalosses)/len(metalosses)\n",
        "    meta_loss.backward()\n",
        "    opt2.step()\n",
        "    # for i,param in enumerate(net2.parameters()):\n",
        "    #     print(param.grad.data)\n",
        "    #     print('****************')\n",
        "       \n",
        "\n",
        "    v_f = net1(v_x, new_params)\n",
        "    loss3 = F.l1_loss(v_f, v_y)\n",
        "    loss3.backward()\n",
        "    opt1.step()\n",
        "\n",
        "    if it % 1000 == 0: \n",
        "        print('Iteration %d -- Outer Loss: %.4f loss1: %.4f' % (it, meta_loss, loss3))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# t_b = math.pi #0\n",
        "\n",
        "# t_x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "# t_y = torch.sin(t_x + t_b)\n",
        "\n",
        "# opt.zero_grad()\n",
        "\n",
        "# with higher.innerloop_ctx(net,opt1, track_higher_grads=False) as (fnet,diffoptim):\n",
        "#     for k in range(n_inner_loop):\n",
        "#         f = fnet(x)\n",
        "#         loss = F.l1_loss(f, y)\n",
        "#         diffoptim.step(loss)\n",
        "\n",
        "\n",
        "\n",
        "#     test_x = torch.arange(-2*math.pi, 2*math.pi, step=0.01).unsqueeze(1)\n",
        "#     test_y = torch.sin(test_x + t_b)\n",
        "\n",
        "#     test_f = fnet(test_x)\n",
        "\n",
        "# plt.plot(test_x.data.numpy(), test_y.data.numpy(), label='sin(x)')\n",
        "# plt.plot(test_x.data.numpy(), test_f.data.numpy(), label='net(x)')\n",
        "# plt.plot(t_x.data.numpy(), t_y.data.numpy(), 'o', label='Examples')\n",
        "# plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dZKd53qLyO_q"
      },
      "source": [
        "#@title\n",
        "#maml with heigher version 2\n",
        "import math\n",
        "import random\n",
        "import torch # v0.4.1\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib as mpl\n",
        "# mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import higher\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def net1(x, params):\n",
        "    x = F.linear(x, params[0], params[1])\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = F.linear(x, params[2], params[3])\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = F.linear(x, params[4], params[5])\n",
        "    return x\n",
        "\n",
        "\n",
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor, params=None):\n",
        "        if params is not None:\n",
        "            x = F.linear(x, params[0], params[1])\n",
        "            x = F.relu(x)\n",
        "\n",
        "            x = F.linear(x, params[2], params[3])\n",
        "            x = F.relu(x)\n",
        "\n",
        "            x = F.linear(x, params[4], params[5])\n",
        "            return x\n",
        "\n",
        "        return self.net(x)\n",
        "\n",
        "net2 = myNet()\n",
        "\n",
        "params = [\n",
        "    torch.Tensor(2, 1).uniform_(-1., 1.).requires_grad_(),\n",
        "    torch.Tensor(2).zero_().requires_grad_(),\n",
        "\n",
        "    torch.Tensor(2, 2).uniform_(-1./math.sqrt(2), 1./math.sqrt(2)).requires_grad_(),\n",
        "    torch.Tensor(2).zero_().requires_grad_(),\n",
        "\n",
        "    torch.Tensor(1, 2).uniform_(-1./math.sqrt(2), 1./math.sqrt(2)).requires_grad_(),\n",
        "    torch.Tensor(1).zero_().requires_grad_()\n",
        "]\n",
        "\n",
        "for i,param in enumerate(net2.parameters()):\n",
        "    param.data = params[i].data\n",
        "    \n",
        "net2.zero_grad()\n",
        "for i,param in enumerate(net2.parameters()):\n",
        "    print(param.grad)\n",
        "    print('****************')\n",
        "    print(params[i])\n",
        "    print('#################')\n",
        "\n",
        "opt1 = torch.optim.SGD(params, lr=1e-2)\n",
        "opt2 = torch.optim.SGD(net2.parameters(), lr=1e-2)\n",
        "n_inner_loop = 5\n",
        "alpha = 3e-2\n",
        "opt3 = torch.optim.SGD(net2.parameters(), lr=alpha)\n",
        "\n",
        "for it in range(10000):\n",
        "    b = 0 if random.choice([True, False]) else math.pi\n",
        "\n",
        "    x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "    y = torch.sin(x + b)\n",
        "\n",
        "    v_x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "    v_y = torch.sin(v_x + b)\n",
        "\n",
        "\n",
        "    \n",
        "    opt1.zero_grad\n",
        "    new_params = params\n",
        "    new_param2 = [param for param in net2.parameters()]\n",
        "    opt2.zero_grad()\n",
        "    with higher.innerloop_ctx(net2,opt3,copy_initial_weights=False) as (fnet,diffoptim):\n",
        "        for k in range(n_inner_loop):\n",
        "            f2 = fnet(x)\n",
        "            loss2 = F.l1_loss(f2, y)\n",
        "            diffoptim.step(loss2)\n",
        "\n",
        "            f1 = net1(x, new_params)\n",
        "            loss1 = F.l1_loss(f1, y)\n",
        "\n",
        "            # create_graph=True because computing grads here is part of the forward pass.\n",
        "            # We want to differentiate through the SGD update steps and get higher order\n",
        "            # derivatives in the backward pass.\n",
        "            grads = torch.autograd.grad(loss1, new_params, create_graph=True)\n",
        "            new_params = [(new_params[i] - alpha*grads[i]) for i in range(len(params))]\n",
        "\n",
        "        if it % 1000 == 0: \n",
        "            print('Iteration %d -- Inner loop %d -- Loss: %.4f  Loss1: %.4f' % (it, k, loss2, loss1))\n",
        "        meta_loss = F.l1_loss(fnet(v_x), v_y)\n",
        "        meta_loss.backward()\n",
        "    opt2.step()\n",
        "    # for i,param in enumerate(net2.parameters()):\n",
        "    #     print(param.grad.data)\n",
        "    #     print('****************')\n",
        "       \n",
        "\n",
        "    v_f = net1(v_x, new_params)\n",
        "    loss3 = F.l1_loss(v_f, v_y)\n",
        "    loss3.backward()\n",
        "    opt1.step()\n",
        "\n",
        "    if it % 1000 == 0: \n",
        "        print('Iteration %d -- Outer Loss: %.4f loss1: %.4f' % (it, meta_loss, loss3))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Z6piw6bjog",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "t_b = math.pi #0\n",
        "\n",
        "t_x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "t_y = torch.sin(t_x + t_b)\n",
        "\n",
        "opt2.zero_grad()\n",
        "\n",
        "with higher.innerloop_ctx(net2,opt3, track_higher_grads=False) as (fnet,diffoptim):\n",
        "    for k in range(n_inner_loop):\n",
        "        f = fnet(x)\n",
        "        loss = F.l1_loss(f, y)\n",
        "        diffoptim.step(loss)\n",
        "\n",
        "\n",
        "\n",
        "    test_x = torch.arange(-2*math.pi, 2*math.pi, step=0.01).unsqueeze(1)\n",
        "    test_y = torch.sin(test_x + t_b)\n",
        "\n",
        "    test_f = fnet(test_x)\n",
        "\n",
        "plt.plot(test_x.data.numpy(), test_y.data.numpy(), label='sin(x)')\n",
        "plt.plot(test_x.data.numpy(), test_f.data.numpy(), label='net(x)')\n",
        "plt.plot(t_x.data.numpy(), t_y.data.numpy(), 'o', label='Examples')\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_e8uIm7uDKY",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "t_b = math.pi #0\n",
        "\n",
        "t_x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "t_y = torch.sin(t_x + t_b)\n",
        "\n",
        "opt1.zero_grad()\n",
        "\n",
        "t_params = params\n",
        "for k in range(n_inner_loop):\n",
        "    t_f = net1(t_x, t_params)\n",
        "    t_loss = F.l1_loss(t_f, t_y)\n",
        "\n",
        "    grads = torch.autograd.grad(t_loss, t_params, create_graph=True)\n",
        "    t_params = [(t_params[i] - alpha*grads[i]) for i in range(len(params))]\n",
        "\n",
        "\n",
        "test_x = torch.arange(-2*math.pi, 2*math.pi, step=0.01).unsqueeze(1)\n",
        "test_y = torch.sin(test_x + t_b)\n",
        "\n",
        "test_f = net1(test_x, t_params)\n",
        "\n",
        "plt.plot(test_x.data.numpy(), test_y.data.numpy(), label='sin(x)')\n",
        "plt.plot(test_x.data.numpy(), test_f.data.numpy(), label='net(x)')\n",
        "plt.plot(t_x.data.numpy(), t_y.data.numpy(), 'o', label='Examples')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMIJi283fmb9",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# raw mamal implementation\n",
        "import math\n",
        "import random\n",
        "import torch # v0.4.1\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib as mpl\n",
        "# mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import higher\n",
        "from torch import Tensor\n",
        "\n",
        "def net1(x, params):\n",
        "    x = F.linear(x, params[0], params[1])\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = F.linear(x, params[2], params[3])\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = F.linear(x, params[4], params[5])\n",
        "    return x\n",
        "\n",
        "\n",
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor, params=None):\n",
        "        if params is not None:\n",
        "            x = F.linear(x, params[0], params[1])\n",
        "            x = F.relu(x)\n",
        "\n",
        "            x = F.linear(x, params[2], params[3])\n",
        "            x = F.relu(x)\n",
        "\n",
        "            x = F.linear(x, params[4], params[5])\n",
        "            return x\n",
        "\n",
        "        return self.net(x)\n",
        "\n",
        "net2 = myNet()\n",
        "\n",
        "params = [\n",
        "    torch.Tensor(2, 1).uniform_(-1., 1.).requires_grad_(),\n",
        "    torch.Tensor(2).zero_().requires_grad_(),\n",
        "\n",
        "    torch.Tensor(2, 2).uniform_(-1./math.sqrt(2), 1./math.sqrt(2)).requires_grad_(),\n",
        "    torch.Tensor(2).zero_().requires_grad_(),\n",
        "\n",
        "    torch.Tensor(1, 2).uniform_(-1./math.sqrt(2), 1./math.sqrt(2)).requires_grad_(),\n",
        "    torch.Tensor(1).zero_().requires_grad_()\n",
        "]\n",
        "\n",
        "for i,param in enumerate(net2.parameters()):\n",
        "    param.data = params[i].data\n",
        "    \n",
        "net2.zero_grad()\n",
        "for i,param in enumerate(net2.parameters()):\n",
        "    print(param.grad)\n",
        "    print('****************')\n",
        "    print(params[i])\n",
        "    print('#################')\n",
        "\n",
        "opt1 = torch.optim.SGD(params, lr=1e-2)\n",
        "opt2 = torch.optim.SGD(net2.parameters(), lr=1e-2)\n",
        "n_inner_loop = 5\n",
        "alpha = 3e-2\n",
        "opt3 = torch.optim.SGD(net2.parameters(), lr=alpha)\n",
        "\n",
        "for it in range(10000):\n",
        "    b = 0 if random.choice([True, False]) else math.pi\n",
        "\n",
        "    x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "    y = torch.sin(x + b)\n",
        "\n",
        "    v_x = torch.rand(4, 1)*4*math.pi - 2*math.pi\n",
        "    v_y = torch.sin(v_x + b)\n",
        "\n",
        "\n",
        "    \n",
        "    opt1.zero_grad\n",
        "    opt2.zero_grad()\n",
        "    metalosses = []\n",
        "    new_params = params\n",
        "    new_params2 = [param for param in net2.parameters()]\n",
        "    \n",
        "    for k in range(n_inner_loop):\n",
        "        f2 = net2(x,new_params2)\n",
        "        loss2 = F.l1_loss(f2, y)\n",
        "        grads2 = torch.autograd.grad(loss2, new_params2, create_graph=True)\n",
        "        new_params2 = [(new_params2[i] - alpha*grads2[i]) for i in range(len(params))]\n",
        "\n",
        "        f1 = net1(x, new_params)\n",
        "        loss1 = F.l1_loss(f1, y)\n",
        "\n",
        "        # create_graph=True because computing grads here is part of the forward pass.\n",
        "        # We want to differentiate through the SGD update steps and get higher order\n",
        "        # derivatives in the backward pass.\n",
        "        grads = torch.autograd.grad(loss1, new_params, create_graph=True)\n",
        "        new_params = [(new_params[i] - alpha*grads[i]) for i in range(len(params))]\n",
        "\n",
        "    if it % 1000 == 0: \n",
        "        print('Iteration %d -- Inner loop %d -- Loss: %.4f  Loss1: %.4f' % (it, k, loss2, loss1))\n",
        "    \n",
        "    # metalosses.append(F.l1_loss(fnet(v_x), v_y))\n",
        "    v_f2 = net2(v_x,new_params2)\n",
        "    meta_loss = F.l1_loss(v_f2, v_y)\n",
        "    meta_loss.backward()\n",
        "    opt2.step()\n",
        "    \n",
        "       \n",
        "\n",
        "    v_f = net1(v_x, new_params)\n",
        "    loss3 = F.l1_loss(v_f, v_y)\n",
        "    loss3.backward()\n",
        "    opt1.step()\n",
        "\n",
        "    if it % 1000 == 0: \n",
        "        print('Iteration %d -- Outer Loss: %.4f loss1: %.4f' % (it, meta_loss, loss3))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "A2LkGpMKnzQW"
      },
      "source": [
        "#@title\n",
        "\"\"\" Residual components of the network\"\"\"\n",
        "\n",
        "\n",
        "class Resblock(nn.Module):\n",
        "\n",
        "    def __init__(self, block_id: int):\n",
        "        super().__init__()\n",
        "        self.block_id = 'resblock%d' % block_id\n",
        "\n",
        "        self.add_module('conv1',\n",
        "                        nn.Conv2d(**model_config['resblocks'][self.block_id]['conv1']))\n",
        "        self.add_module('conv2',\n",
        "                        nn.Conv2d(**(model_config['resblocks'][self.block_id]['conv2'])))\n",
        "        self.add_module('reducer', nn.Conv2d(**model_config['reducer'][self.block_id]))\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        x_init = x\n",
        "\n",
        "        for i, block in enumerate(self.children()):\n",
        "            if i == 2:\n",
        "                break\n",
        "            x = block(x)\n",
        "\n",
        "        return x + block(x_init)\n",
        "\n",
        "\"\"\"Implements the model described in arxiv.2008.00247\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"MetaDRN architectured described in arxiv.2008.00247\"\"\"\n",
        "\n",
        "class MetaDRN1(nn.Module):\n",
        "    def __init__(self, algo, init_meta_learner_lr=1e-3):\n",
        "        super().__init__()\n",
        "        # self.params = params\n",
        "        self.algo = algo\n",
        "        self.init_meta_learner_lr = init_meta_learner_lr\n",
        "        \n",
        "        # Definet the network\n",
        "        self.head = nn.Sequential()\n",
        "        self.head.add_module(\"conv1\", nn.Conv2d(**model_config[\"head\"][\"conv1\"]))\n",
        "        self.head.add_module(\"bn1\", nn.BatchNorm2d(**model_config[\"head\"][\"bn1\"]))\n",
        "        self.head.add_module(\"lr1\", nn.LeakyReLU())\n",
        "        self.head.add_module(\"conv2\", nn.Conv2d(**model_config[\"head\"][\"conv2\"]))\n",
        "        self.head.add_module(\"bn2\", nn.BatchNorm2d(**model_config[\"head\"][\"bn2\"]))\n",
        "        self.head.add_module(\"lr2\", nn.LeakyReLU())\n",
        "\n",
        "        self.resblock1 = nn.Sequential()\n",
        "        self.resblock1.add_module(\"resblock1\", Resblock(1))\n",
        "        \n",
        "        self.resblock2 = nn.Sequential()\n",
        "        self.resblock2.add_module(\"resblock2\", Resblock(2))\n",
        "        \n",
        "        self.resblock3 = nn.Sequential()\n",
        "        self.resblock3.add_module(\"resblock3\", Resblock(3))\n",
        "\n",
        "        self.degrid = nn.Sequential()\n",
        "        self.degrid.add_module(\"conv1\", nn.Conv2d(**model_config[\"degrid\"][\"conv1\"]))\n",
        "        self.degrid.add_module(\"conv2\", nn.Conv2d(**model_config[\"degrid\"][\"conv2\"]))\n",
        "\n",
        "        self.upsample = nn.Sequential(\n",
        "            OrderedDict([(\"conv1\", nn.Conv2d(**model_config[\"upsample\"][\"conv\"])),\n",
        "                         (\"pixel_shuffle\",\n",
        "                          nn.PixelShuffle(**model_config[\"upsample\"][\"pixel_shuffle\"]))]))\n",
        "        \n",
        "        if algo == 'meta-sgd':\n",
        "            self.task_lr = OrderedDict()\n",
        "\n",
        "    def forward(self, x, params=None):\n",
        "        if params is not None:\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['head.conv1.weight'], \n",
        "                         bias=params['head.conv1.bias'], \n",
        "                         stride=2, \n",
        "                         padding=1, \n",
        "                         dilation=1)\n",
        "            # with torch.no_grad():\n",
        "            #     self.state_dict()['head.bn1.num_batches_tracked'].data += 1\n",
        "            #     rv, rm = torch.var_mean(x, dim=(0,2,3))\n",
        "            x = F.batch_norm(x,\n",
        "                             running_mean=self.state_dict()['head.bn1.running_mean'], \n",
        "                             running_var=self.state_dict()['head.bn1.running_var'], \n",
        "                             weight=params['head.bn1.weight'], \n",
        "                             bias=params['head.bn1.bias'],\n",
        "                             training=True)\n",
        "            x = F.leaky_relu(x)\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['head.conv2.weight'], \n",
        "                         bias=params['head.conv2.bias'], \n",
        "                         stride=1, \n",
        "                         padding=1, \n",
        "                         dilation=1)\n",
        "            # with torch.no_grad():\n",
        "            #     self.state_dict()['head.bn2.num_batches_tracked'].data += 1\n",
        "            #     rv, rm = torch.var_mean(x, dim=(0,2,3))\n",
        "                \n",
        "            x = F.batch_norm(x,\n",
        "                             running_mean=self.state_dict()['head.bn2.running_mean'], \n",
        "                             running_var=self.state_dict()['head.bn2.running_var'], \n",
        "                             weight=params['head.bn2.weight'], \n",
        "                             bias=params['head.bn2.bias'],\n",
        "                             training=True)\n",
        "            x_init = x\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['resblock1.resblock1.conv1.weight'], \n",
        "                         bias=params['resblock1.resblock1.conv1.bias'], \n",
        "                         stride=2, \n",
        "                         padding=1, \n",
        "                         dilation=1)\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['resblock1.resblock1.conv2.weight'], \n",
        "                         bias=params['resblock1.resblock1.conv2.bias'], \n",
        "                         stride=1, \n",
        "                         padding=1, \n",
        "                         dilation=1)\n",
        "            x = x + F.conv2d(x_init, \n",
        "                             weight=params['resblock1.resblock1.reducer.weight'], \n",
        "                             bias=params['resblock1.resblock1.reducer.bias'], \n",
        "                             stride=2, \n",
        "                             padding=0, \n",
        "                             dilation=1)\n",
        "            x_init = x\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['resblock2.resblock2.conv1.weight'], \n",
        "                         bias=params['resblock2.resblock2.conv1.bias'], \n",
        "                         stride=1, \n",
        "                         padding=1, \n",
        "                         dilation=1)\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['resblock2.resblock2.conv2.weight'], \n",
        "                         bias=params['resblock2.resblock2.conv2.bias'], \n",
        "                         stride=1, \n",
        "                         padding=2, \n",
        "                         dilation=2)\n",
        "            x = x + F.conv2d(x_init, \n",
        "                             weight=params['resblock2.resblock2.reducer.weight'], \n",
        "                             bias=params['resblock2.resblock2.reducer.bias'], \n",
        "                             stride=1, \n",
        "                             padding=0, \n",
        "                             dilation=1)\n",
        "            x_init = x\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['resblock3.resblock3.conv1.weight'], \n",
        "                         bias=params['resblock3.resblock3.conv1.bias'], \n",
        "                         stride=1, \n",
        "                         padding=2, \n",
        "                         dilation=2)\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['resblock3.resblock3.conv2.weight'], \n",
        "                         bias=params['resblock3.resblock3.conv2.bias'], \n",
        "                         stride=1, \n",
        "                         padding=4, \n",
        "                         dilation=4)\n",
        "            x = x + F.conv2d(x_init, \n",
        "                             weight=params['resblock3.resblock3.reducer.weight'], \n",
        "                             bias=params['resblock3.resblock3.reducer.bias'], \n",
        "                             stride=1, \n",
        "                             padding=0, \n",
        "                             dilation=1)\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['degrid.conv1.weight'], \n",
        "                         bias=params['degrid.conv1.bias'], \n",
        "                         stride=1, \n",
        "                         padding=2, \n",
        "                         dilation=2)\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['degrid.conv2.weight'], \n",
        "                         bias=params['degrid.conv2.bias'], \n",
        "                         stride=1, \n",
        "                         padding=1, \n",
        "                         dilation=1)\n",
        "            x = F.conv2d(x, \n",
        "                         weight=params['upsample.conv1.weight'], \n",
        "                         bias=params['upsample.conv1.bias'], \n",
        "                         stride=1, \n",
        "                         padding=1, \n",
        "                         dilation=1)\n",
        "            x = F.pixel_shuffle(x,4)\n",
        "            return x\n",
        "        return self.upsample(self.degrid(self.resblock3(self.resblock2(self.resblock1(self.head(x))))))\n",
        "    \n",
        "    def cloned_state_dict(self):\n",
        "        cloned_state_dict = {\n",
        "            key: val.clone()\n",
        "            for key, val in self.state_dict().items()\n",
        "        }\n",
        "        return cloned_state_dict\n",
        "\n",
        "    def define_task_lr_params(self):\n",
        "        for key, val in self.named_parameters():\n",
        "            self.task_lr[key] = nn.Parameter(\n",
        "                self.init_meta_learner_lr * torch.ones_like(val, requires_grad=True))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL7flBTKkohD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ0e_S7Jkvfg"
      },
      "source": [
        "model = MetaDRN1(\"maml\")\n",
        "model.cuda()\n",
        "model_params = list(model.parameters()) #+ list(model.task_lr.values())\n",
        "meta_optim = torch.optim.Adam(model_params, lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AzBW_4MXBjOn"
      },
      "source": [
        "#@title\n",
        "def train1(net, loader, epoch=0, writer=None):\n",
        "    net.train()\n",
        "    qry_losses = []\n",
        "    qry_ious = []\n",
        "    pbar = tqdm(loader)\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        (train_x, train_y), (test_x, test_y) = split_batch(batch, algo, 'train')\n",
        "\n",
        "        task_size = train_x.size(0)\n",
        "        adapted_state_dicts = []\n",
        "        for task_num in range(task_size):\n",
        "            spt_x = train_x[task_num,:,:,:,:,:].view(-1, *train_x.shape[3:]).cuda() \n",
        "            spt_y = train_y[task_num,:,:,:,:,:].view(-1, *train_y.shape[3:]).cuda()\n",
        "            # compute model output and loss\n",
        "            Y_sup_hat = net(spt_x)\n",
        "            loss = F.cross_entropy(Y_sup_hat, spt_y.squeeze().long())\n",
        "\n",
        "            # clear previous gradients, compute gradients of all variables wrt loss\n",
        "            def zero_grad(params):\n",
        "                for p in params:\n",
        "                    if p.grad is not None:\n",
        "                        p.grad.zero_()\n",
        "\n",
        "            # NOTE if we want approx-MAML, change create_graph=True to False\n",
        "            zero_grad(net.parameters())\n",
        "            grads = torch.autograd.grad(loss, net.parameters(), create_graph=True)\n",
        "\n",
        "            # performs updates using calculated gradients\n",
        "            # we manually compute adpated parameters since optimizer.step() operates in-place\n",
        "            adapted_state_dict = net.cloned_state_dict()\n",
        "            adapted_params = OrderedDict()\n",
        "            for (key, val), grad in zip(model.named_parameters(), grads):\n",
        "                # NOTE Here Meta-SGD is different from naive MAML\n",
        "                # Also we only need single update of inner gradient update\n",
        "                task_lr = 1e-3 #model.task_lr[key]\n",
        "                adapted_params[key] = val - task_lr * grad \n",
        "                adapted_state_dict[key] = adapted_params[key]\n",
        "            adapted_state_dicts.append(adapted_state_dict)\n",
        "\n",
        "        meta_loss = 0 #torch.tensor(0).cuda()\n",
        "        for task_num in range(task_size):\n",
        "            qry_x = test_x[task_num,:,:,:,:,:].view(-1, *test_x.shape[3:]).cuda() \n",
        "            qry_y = test_y[task_num,:,:,:,:,:].view(-1, *test_y.shape[3:]).cuda()\n",
        "            a_dict = adapted_state_dicts[task_num]\n",
        "            Y_meta_hat = net(qry_x, a_dict)\n",
        "            loss_t = F.cross_entropy(Y_meta_hat, qry_y.squeeze().long())\n",
        "            meta_loss += loss_t\n",
        "        meta_loss /= float(task_size)\n",
        "        meta_optim.zero_grad()\n",
        "        meta_loss.backward()\n",
        "        meta_optim.step()\n",
        "        pbar.set_description(\"Epoch: %d, Training Loss: %.2f, mIoU: %.2f, time: %s\" %\n",
        "                            (epoch, meta_loss, 0, time.strftime('%X')))\n",
        "        \n",
        "        \n",
        "\n",
        "    # qry_loss_meta = sum(qry_losses) / len(qry_losses)\n",
        "    # qry_iou_meta = sum(qry_ious) / len(qry_ious)\n",
        "    # print(\"loss: {} iou: {}\".format(qry_iou_meta, qry_iou_meta))\n",
        "    # if writer is not None:\n",
        "    #     writer.add_scalar('training loss', qry_loss_meta, epoch)\n",
        "    #     writer.add_scalar('training mIoU', qry_iou_meta, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad8l_pULFwO4"
      },
      "source": [
        "train1(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBca2H-cF4eE"
      },
      "source": [
        "batch = train_loader.__iter__().next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spNFSaUOMasn",
        "outputId": "3b2d890a-a2ec-428d-8175-29bf554235d9"
      },
      "source": [
        "batch.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 5, 4, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8qlCWCjNexe"
      },
      "source": [
        "(spt_x, spt_y), (qry_x, qry_y) = split_batch(batch, algo, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHbTMurOcXqV",
        "outputId": "8a8d4d21-5dc3-4b86-a8f4-d988b5b41525"
      },
      "source": [
        "spt_x.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 5, 3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdoyCCz5cbt9",
        "outputId": "36dcd2a4-01a3-4f20-ea37-6a4ada4f2965"
      },
      "source": [
        "spt_y.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 5, 1, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "WMkjxzIEcm7Z"
      },
      "source": [
        "#@title\n",
        "#maml with heigher version 3 meta-sgd\n",
        "import math\n",
        "import random\n",
        "import torch # v0.4.1\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib as mpl\n",
        "# mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import higher\n",
        "from torch import Tensor\n",
        "\n",
        "seed_everything(1971)\n",
        "\n",
        "from torch.optim import Optimizer \n",
        "\n",
        "\n",
        "class dSGD(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(dSGD, self).__init__(params, defaults)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "class DifferentiableSGD1(DifferentiableOptimizer):\n",
        "    r\"\"\"A differentiable version of the SGD optimizer.\n",
        "    This optimizer creates a gradient tape as it updates parameters.\"\"\"\n",
        "\n",
        "    def _update(self, grouped_grads: _GroupedGradsType, **kwargs) -> None:\n",
        "        # print(\"%%%%%%%%%\")\n",
        "        # print(self.task_lr)\n",
        "        # print(\"%%%%%%%%%\")\n",
        "        zipped = zip(self.param_groups, grouped_grads)\n",
        "        for group_idx, (group, grads) in enumerate(zipped):\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for p_idx, (p, g) in enumerate(zip(group['params'], grads)):\n",
        "                if g is None:\n",
        "                    continue\n",
        "\n",
        "                # if weight_decay != 0:\n",
        "                #     g = _add(g, weight_decay, p)\n",
        "                # if momentum != 0:\n",
        "                #     param_state = self.state[group_idx][p_idx]\n",
        "                #     if 'momentum_buffer' not in param_state:\n",
        "                #         buf = param_state['momentum_buffer'] = g\n",
        "                #     else:\n",
        "                #         buf = param_state['momentum_buffer']\n",
        "                #         buf = _add(buf.mul(momentum), 1 - dampening, g)\n",
        "                #         param_state['momentum_buffer'] = buf\n",
        "                #     if nesterov:\n",
        "                #         g = _add(g, momentum, buf)\n",
        "                #     else:\n",
        "                #         g = buf\n",
        "\n",
        "                # group['params'][p_idx] = _add(p, -group['lr'], g)\n",
        "                print(g)\n",
        "                group['params'][p_idx] = _add(p, -self.task_lr[p_idx], g)\n",
        "\n",
        "    def store_task_lr(self,task_lr):\n",
        "        self.task_lr = task_lr        \n",
        "    \n",
        "\n",
        "register_optim(dSGD, DifferentiableSGD1)\n",
        "\n",
        "def net1(x, params):\n",
        "    x = F.linear(x, params[0], params[1])\n",
        "    return x\n",
        "\n",
        "\n",
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(1, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor, params=None):\n",
        "        if params is not None:\n",
        "            x = F.linear(x, params[0], params[1])\n",
        "            \n",
        "            return x\n",
        "\n",
        "        return self.net(x)\n",
        "\n",
        "net2 = myNet()\n",
        "\n",
        "params = [\n",
        "    torch.Tensor(1, 1).uniform_(-1., 1.).requires_grad_(),\n",
        "    torch.Tensor(1).zero_().requires_grad_(),\n",
        "]\n",
        "\n",
        "for i,param in enumerate(net2.parameters()):\n",
        "    if i==0:\n",
        "        param.data = torch.tensor(5.0).view(1,1)\n",
        "    else:\n",
        "        param.data = torch.tensor(2.0)\n",
        "    \n",
        "net2.zero_grad()\n",
        "\n",
        "alpha = 3e-2\n",
        "opt1 = torch.optim.SGD(params, lr=1e-2)\n",
        "\n",
        "alpa = nn.Parameter(torch.tensor(alpha, requires_grad=True))\n",
        "beta = nn.Parameter(torch.tensor(alpha, requires_grad=True))\n",
        "task_lr = OrderedDict()\n",
        "task_lr['net.0.weight'] = alpa\n",
        "task_lr['net.0.bias'] = beta\n",
        "\n",
        "opt2 = torch.optim.SGD(list(net2.parameters())+list(task_lr.values()), lr=1e-2)\n",
        "n_inner_loop = 1\n",
        "\n",
        "print(task_lr)\n",
        "print('********')\n",
        "# opt3 = torch.optim.SGD(net2.parameters(), lr=alpha)\n",
        "opt3 = dSGD(net2.parameters(), lr=alpha)\n",
        "# opt3.store_task_lr(list(task_lr.values()))\n",
        "\n",
        "def grad_cb(x):\n",
        "    # print(x)\n",
        "    # x[0].data = x[0].data*alpa\n",
        "    # print(x)\n",
        "    return x\n",
        "\n",
        "for it in range(1):\n",
        "    b = 0 if random.choice([True, False]) else math.pi\n",
        "\n",
        "    x = torch.tensor(3.0).view(1,1)#torch.rand(1, 1)*4*math.pi - 2*math.pi\n",
        "    y = x**2#torch.sin(x + b)\n",
        "\n",
        "    v_x = torch.tensor(7.0).view(1,1)#torch.rand(1, 1)*4*math.pi - 2*math.pi\n",
        "    v_y = v_x**2 #torch.sin(v_x + b)\n",
        "\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(v_x)\n",
        "    print(v_y)\n",
        "    \n",
        "    opt1.zero_grad\n",
        "    new_params = params\n",
        "    new_param2 = [param for param in net2.parameters()]\n",
        "    opt2.zero_grad()\n",
        "    \n",
        "    print('alpa grad {}'.format(alpa.grad))\n",
        "    with higher.innerloop_ctx(net2,opt3,copy_initial_weights=False) as (fnet,diffoptim):\n",
        "        for k in range(n_inner_loop):\n",
        "            f2 = fnet(x)\n",
        "            loss2 = F.mse_loss(f2, y)\n",
        "            if k==0:\n",
        "                diffoptim.store_task_lr(list(task_lr.values()))\n",
        "            up = diffoptim.step(loss2)#,grad_callback=grad_cb)\n",
        "            print(up)\n",
        "\n",
        "            # f1 = net1(x, new_params)\n",
        "            # loss1 = F.mse_loss(f1, y)\n",
        "\n",
        "            # create_graph=True because computing grads here is part of the forward pass.\n",
        "            # We want to differentiate through the SGD update steps and get higher order\n",
        "            # derivatives in the backward pass.\n",
        "            # grads = torch.autograd.grad(loss1, new_params, create_graph=True)\n",
        "            # new_params = [(new_params[i] - alpha*grads[i]) for i in range(len(params))]\n",
        "\n",
        "        if True: \n",
        "            print('Iteration %d -- Inner loop %d -- Loss: %.4f  Loss1: %.4f' % (it, k, loss2, loss1))\n",
        "        for i,param in enumerate(net2.parameters()):\n",
        "            print(param.grad)\n",
        "            print('****************')\n",
        "        meta_loss = F.mse_loss(fnet(v_x), v_y)\n",
        "        print('alpa grad {}'.format(alpa.grad))\n",
        "        print('beta grad {}'.format(beta.grad))\n",
        "        meta_loss.backward()\n",
        "        print('alpa grad {}'.format(alpa.grad))\n",
        "        print('beta grad {}'.format(beta.grad))\n",
        "        for i,param in enumerate(net2.parameters()):\n",
        "            print(param.grad)\n",
        "            print('****************')\n",
        "        # print(task_lr)\n",
        "        # print('########')\n",
        "    opt2.step()\n",
        "    # print(task_lr)\n",
        "    # print('$$$$$$$')\n",
        "    # for i,param in enumerate(net2.parameters()):\n",
        "    #     print(param.grad.data)\n",
        "    #     print('****************')\n",
        "       \n",
        "\n",
        "    # v_f = net1(v_x, new_params)\n",
        "    # loss3 = F.l1_loss(v_f, v_y)\n",
        "    # loss3.backward()\n",
        "    # opt1.step()\n",
        "\n",
        "    if True: \n",
        "        print('Iteration %d -- Outer Loss: %.4f loss1: %.4f' % (it, meta_loss, loss3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkpKpQUU9ULx",
        "outputId": "7b8761a4-828c-4c36-ed98-4a1b1c816e17"
      },
      "source": [
        "beta.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1L2iCjzDZv9"
      },
      "source": [
        "#@title\n",
        "def test(net, loader, epoch=0, writer=None):\n",
        "    net.train()\n",
        "    qry_losses = []\n",
        "    qry_ious = []\n",
        "    pbar = tqdm(loader)\n",
        "    \n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        (spt_x, spt_y), (qry_x, qry_y ) = split_batch(batch, algo, 'test')\n",
        "       \n",
        "        \n",
        "        spt_x, spt_y = spt_x.view(-1, *spt_x.shape[3:]).cuda(), spt_y.view(\n",
        "           -1, *spt_y.shape[3:]).cuda()\n",
        "        qry_x, qry_y = qry_x.view(-1, *qry_x.shape[3:]).cuda(), qry_y.view(\n",
        "           -1, *qry_y.shape[3:]).cuda()\n",
        "\n",
        "        \n",
        "        if algo in ['maml', 'fomaml', 'meta-sgd']:\n",
        "            \n",
        "            with higher.innerloop_ctx(net, learner_optim, \n",
        "                                        copy_initial_weights=True, \n",
        "                                        track_higher_grads=False) as (fnet, diffoptim):\n",
        "                for i in range(train_config[algo]['train_steps']):\n",
        "                    pred = fnet(spt_x)\n",
        "                    loss = F.cross_entropy(pred, spt_y.squeeze().long())\n",
        "                    if i==0 and algo == 'meta-sgd':\n",
        "                        diffoptim.store_task_lr(list(net.task_lr.values()))\n",
        "                    diffoptim.step(loss)\n",
        "                with torch.no_grad():\n",
        "                    add_hook_to_Model(fnet)\n",
        "                    qry_logits = fnet(qry_x)\n",
        "                    qry_loss = F.cross_entropy(qry_logits, qry_y.squeeze().long())\n",
        "                    qry_losses.append(qry_loss.detach())\n",
        "                    qry_iou = iou(torch.argmax(qry_logits, dim=1), qry_y.squeeze().long())\n",
        "                    qry_ious.append(qry_iou)\n",
        "                \n",
        "                    pbar.set_description(\"Epoch: %d, testing Loss: %.2f, mIoU: %.2f, time: %s\" %\n",
        "                                (epoch, qry_losses[-1], qry_ious[-1], time.strftime('%X')))\n",
        "                    out_y = torch.argmax(qry_logits, dim=1, keepdim=True)\n",
        "                    act_fig = get_activationFig(spt_x.clone(), spt_y.clone(), qry_x.clone(), out_y.clone(), activation)\n",
        "                    fig = get_matplotFig(spt_x.clone(), spt_y.clone(), qry_x.clone(), out_y.clone())\n",
        "                    \n",
        "                    if writer is not None:\n",
        "                        writer.add_figure(\"testing_images\", fig, batch_idx)\n",
        "                        writer.add_figure(\"testing_activation_images\", act_fig, batch_idx)\n",
        "        elif algo == 'reptile':\n",
        "            weights_before = deepcopy(net.state_dict())\n",
        "            for _ in range(train_config[algo]['train_steps']):\n",
        "                    pred = net(spt_x)\n",
        "                    loss = F.cross_entropy(pred, spt_y.squeeze().long())\n",
        "                    net.zero_grad()\n",
        "                    loss.backward()\n",
        "                    learner_optim.step()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                add_hook_to_Model(net)\n",
        "                qry_logits = net(qry_x)\n",
        "                qry_loss = F.cross_entropy(qry_logits, qry_y.squeeze().long())\n",
        "                qry_losses.append(qry_loss.detach())\n",
        "                qry_iou = iou(torch.argmax(qry_logits, dim=1), qry_y.squeeze().long())\n",
        "                qry_ious.append(qry_iou)\n",
        "                pbar.set_description(\"Epoch: %d, testing Loss: %.2f, mIoU: %.2f, time: %s\" %\n",
        "                                (epoch, qry_losses[-1], qry_iou, time.strftime('%X')))\n",
        "                out_y = torch.argmax(qry_logits, dim=1, keepdim=True)\n",
        "                act_fig = get_activationFig(spt_x.clone(), spt_y.clone(), qry_x.clone(), out_y.clone(), activation)\n",
        "                fig = get_matplotFig(spt_x.clone(), spt_y.clone(), qry_x.clone(), out_y.clone())\n",
        "                \n",
        "                if writer is not None:\n",
        "                    writer.add_figure(\"testing_images\", fig, batch_idx)\n",
        "                    writer.add_figure(\"testing_activation_images\", act_fig, batch_idx)\n",
        "\n",
        "            net.load_state_dict(weights_before)\n",
        "\n",
        "    qry_loss_epoch = sum(qry_losses) / len(qry_losses)\n",
        "    qry_iou_epoch= sum(qry_ious) / len(qry_ious)\n",
        "    print(\"loss: {} iou: {}\".format(qry_loss_epoch, qry_iou_epoch))\n",
        "    if writer is not None:\n",
        "        writer.add_scalar('testing loss', qry_loss_epoch, epoch)\n",
        "        writer.add_scalar('testing mIoU', qry_iou_epoch, epoch)\n",
        "            \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}